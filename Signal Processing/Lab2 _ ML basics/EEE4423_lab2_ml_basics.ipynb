{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### EEE4423: Signal Processing Lab\n",
    "\n",
    "# LAB \\#2: Machine Learning Basics:\n",
    "## Linear Regression, Logistic Regression, Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2019-01-08 14:37:42.332042\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with PyTorch\n",
    "\n",
    "## 1. About Linear Regression\n",
    "\n",
    "\n",
    "### 1.1 Simple Linear Regression Basics\n",
    "- Allows us to understand **relationship** between two **continuous variables**\n",
    "- Example\n",
    "    - x: independent variable\n",
    "        - Weight\n",
    "    - y: dependent variable\n",
    "        - Height\n",
    "- $y=\\alpha x + \\beta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Example of simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plot for simple linear regression**\n",
    "\n",
    "Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates 50 random x and y numbers\n",
    "np.random.seed(1)\n",
    "n = 50\n",
    "x = np.random.randn(n)\n",
    "y = x * np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes the dots colorful\n",
    "colors = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3WlwHOed5/nvk5l134XCDYLgfVMkBR3WaVGHKUvutt3jXss9ve21Y+Sd7dmZnuiNnenw+43dmI3d2Yh2R4+32zM9vT6nfcmWZeu2bkqkKIr3feEGARRQ95XPvgAJskRIIIUCEqj6f/xCrkJV5h/Fqh+eevKfTyqtNUIIIeqH4XQBQgghakuCXQgh6owEuxBC1BkJdiGEqDMS7EIIUWck2IUQos5IsAshRJ2RYBdCiDojwS6EEHXGcmKniURC9/T0OLFrIYRYtvbv339Za9081+McCfaenh727dvnxK6FEGLZUkpduJnHyVSMEELUGQl2IYSoMxLsQghRZyTYhRCizkiwCyFEnXGkK0Y4L5stcPRIP4ODSZqbw2zZ2kko5HO6LCFEDUiwN6DJZJbv/39vkU7l8HhdHDvaz7t7z/DUn9xDc3PI6fKEEPMkUzEN6K23TpLNFmhtixCN+mltjVCpVPj9q8ecLk0IUQMS7A3o5IkholF/1X3RaIAzp4epVGyHqhJC1IoEewPy+dyUy5Wq+yqVCh6PC8NQDlUlhKgVCfYGdMedq5mYyMyMzm1bc3k0Te+dq1BKgl2I5U4Onjag7bd1Mz6e5v3950EptG1z285u7r57rdOlCbFoJkamOPjGcYYvjtG2MsFt928kmqiP5gGltV70nfb29mpZBMx56VSe5GSWUMhLJOKf+wlC1ImRvjF+9H8/R6VcwRvwkMsUcLlMnvrLJ0i0x5wu72MppfZrrXvnepxMxTSwYMhLV1dcQl00nNef2Q8KEh0xghE/zR0xbFvzxq/ed7q0mpBgF0I0FK0154/1E2mqnnaJJEKcP9rvUFW1JXPsYlFM5HK8dPYMh0aG8VkuHljZw11dXZiGjC3E4lJKEYwEKOZLeP3umfuL+RLBaH18e5VPlVhw6WKRv3nvXd7r7yfgcmNrm/929DDPnjzpdGmiQd3x6FYmRiYpl6bbfsulCsnRKe58dJvDldWGjNjFgjs4NEgyl6MzHAbAbZp0hSxev3iBB3t6iHi9DlcoGs2O+zeSTeXZ9+JhtK1RhuL+P7idrZ9Z53RpNSHBLhbcxclJvFb1W800DAwFY7mcBLtYdIZhcN+Tu+jdvYX0ZJZQNIDH5577icuETMWIBdceDFGoVJ/pamuN1pqohLpwkNfvIdEeq6tQBwl2sQh2tbfjd7kYzWSwtaZYqdA3NcWu9g7iPlkqWIhak2AXCy7s9fI/3nEHa+JxBtMpUoUCj61Zw5c3b3G6NCHqksyxi0XRFgzxjV23U7ZtDKUwZE0aIRaMBLtYVJb0rQux4ORTJoQQdUaCXQgh6owEuxBC1BkJdiGEqDMS7EIIUWck2IUQos5IsAshRJ2RYBdCiDojwS6EEHWmJsGulPqeUmpEKXW4FtsTQgjx6dVqxP5fgD012pYQQoh5qEmwa61fA8ZrsS0hhBDzI3PsQghRZxYt2JVSTyul9iml9o2Oji7WboUQouEsWrBrrb+rte7VWvc2Nzcv1m6FEKLhyFSMEELUmVq1O/4QeBvYoJTqU0p9sxbbFULUVqacYTg/TKacdboUsYBqcgUlrfVTtdiOEGJhVHSFty/v5VjqOAoFaDaHN3N3050YSr641xu5NJ4QDeBQ8jBHpo6ScDdhKANb2xyaPETICrItutXp8kSNyZ9qIRrAoakjRF2RmdG5oQwiVoQPJ+Vk8Xokwd6gyrZNplTE1trpUsQC01pTqBSwVPUXdMuwKNh5h6oSC0mmYhqMrTWvD5zlxf7T5Msloh4fT67cxG2JDqdLEwtEKcXKQDd92T6irujM/VPlKXr8Pc4VJhaMjNgbzOuD5/jFuSMELTedgQho+IcT+zmVvOx0aWIB3RG7HUtZjBXHSJVSXC6O4VZubo/vdLo0sQBkxN5AKrbNS32naPWH8JjT//QBl5uiXeHl/lOsiyYcrlAslKg7ype7vsSp1CnGimM0e5pZG1yD3/I7XZpYABLsDaRoV8iVS8Q91R9mv+ViJJdxqCqxWAKWnx2x25wuQywCmYppIF7ToskbIFUqVN2fLOZZHY47VJUQotYk2BuIUoov9GwiWcwxns9SqJQZyaVQwO7OtU6XJ4SoEQn2BrMl3sb/tOUeukNRytpmW1M7/3r7fbQHwk6XVqVkp8mWh6nootOlCLHsyBx7A1oTaWJNpMnpMmZV0UUupp7jcu4DUGBgsSL4GM2+XpRSTpcnxLIgwS6WlL70i4zk9hOw2lHKoKKLnE89g8eMEvGsc7o8IZYFCXaxZFTsAqO5/fitVtSVU99N5cYyggzl3pFgXybKpQqH3j7Fh2+dpFK22XLXGnbctwGPz+10aQ1D5tjFklHRRWwqGB859d1UboqVKYeqErdCa83vvv8mL/7kHQq5IpVyhdd++T6/+H9foVKxnS6vYUiwiyXDZQTwmnFKdrrq/oI9SdSz3qGqxK0Y7Z/g+IFztHU34Q968QU8tHXH6Ts9xKWTQ06X1zAk2MWSoZTByuATVzpiRihWpsiUBvEYEVp9dztdnrgJY8OTgKo60K3U9O3RgYlFrWWyOMLBiRd5e/SnnEnto1jJLer+nSRz7GJJiXjWsCX+LUaz+8lVxmh1r6LZtxOXEXS6NHETgmEfs/UuaSAcDyxaHcO5M7w7/isMTCzDzeXJi1zIHubexB/jMet/GQUJdrHk+K02VoafcLqMJadkZxnPn6Gs84RcHYRcHTe0gJbtEqCxDGcOVHasbqG5K87owARNrRFQiuToFOF4gFWbOhelBltX+DD5Cj4jiNv0AeAzg0wWR7iQ+ZD14fr/9ifBLsQykCoNcGj8h5TtPNNDYk2r9zbWRT6PoUzylTSHk68xkDuNRtPuW83WyIP4rcU98cw0Db78rYd55WfvceqDC2it6dnUye4/uhO317UoNeQqaQp2mrCruep+rxlkOH9Ogl0I4TytbY4nf4HCJGC10ndwijNvjJHJ/o57HlDcs/th3kn9glRpnKAVRwHD+QtMlX7OQy1fwzQWJ1CvCoR9PPn1ByjmS2itF73N0aXcgMLWFQxlztxf1kV8ZmhRa3GKBHuDs7XNUH6YbDlDxB0h4U7IGZ5LTLY8Rq6SJGi1cuiZIY78dgRf1IVtGLz0g9e4eHaY6B9OEfO2zjwnZMWZLI0wUrhIu2+NI3Uv1gj9hv2aPrr8m7mYOUTE1YxSBiW7SMnO0xNsjNUtJdgbWK6S47dDz3O5MAYaNJpVgZU81PJZLEPeGkvF1T+0mfEix168TKzbh2EqSnYZbzTA8NAg5oQm1t5a9TytNflKerZN1r0tkQewdYX+3HEUClO52BHbQ8KzwunSFoV8ehvY3rH3GCuMkXBPrxujteZs5jytU8fYHt3mcHXiKp/ZRMBKcHFgGGWAYSq01ti6TMjVRrqQJTM1im7TM38EtNaAImg15nLMLsPDrvgeNlfuo2jnCZiRRZ+ScpL0sTeosl3mTPoMMVds5j6lFGErxPGp4w5WJj5KKcXGyJfwhTwUy1kKlRQlO0PEvQKvGaMy5iZhdpMsj1C0cxTtPJOlEZq93SQ8i9OJslR5zSBhV6KhQh1kxN6w9JX/fZRCYc9y/6fej9YyZ18DAVczj+z8C/q2/AODZ0ZpW9GO2wyQGk/j8bh5dNNXuOw6y4XMYTQ2WyL3syq4fWbNnU8jmy1SKJYIh3yYpowB55LKl+ibyNE/kaNvIkt/Mjd9O5nj3+/ZyD1rF+/SkxLsDcpluFgZWMml7KWZUbvWmqlyit747fPe/tmjfbz17AcM943R0hnn3id2sHpLY8xvLhTLcPPUv/kTXvzH1zj1/jm0ztO8Is7nvv4Q0XiUKLtYG9o17/0UCiWe//1RjhwbQAPBgIfPPbSF9Wta53xuvdJaM5mbDu6+jwb3ldtT+XLVczyWQVfMR2fMj2Es7uBGTc/FLa7e3l69b9++Rd+vqJYqpfjN4G+ZKk0Barr/2dvG59ofxT2PE1zOHe3nn/7mBUIRP4GIj8xUjnQyy5f/5cMS7jWSTeUolyqEYoGafyP6xXMHOHpikJZECMMwyOaKpNJ5vvG1e2ltXloXZKkVrTVjmeLsI+4rtzPFStVzAm6TrpifzphvOsCjvqrbTQF3zf9tlFL7tda9cz1ORuwNLOQK8eWuL9KX6ydVShF3x+jwdWDM4+s7wFu/OUAw4icYnT51OxiZ/u+bvz4gwV4j/pBvQbabSuc5dnKIlkR4ZpTp97nJ5gocOHSRPbu3Lsh+F5pta0bThY8dbfcnc+RL1atPhr0WnTE/3U1+PrOmia4rgd0V89MV8xHxuZbsNKMEe4NzGS5WBXpqus2R/gma2iJV9wXCPkb6x6vm3Een0rx85Aynh8aI+Lw8sGkV27rbluyHpRFkc0WU4oapA7fLRXJy6S6iVa7YDKcK9I1/JLSTWfqvzHOXKtWzE/GAm86oj3UtIR7a0DIzbTL9Xx9hh/rwa0GCXdRcS2eMqYksoei1xZYyUzlaOuMzoT2ezvK3L+6lXLGJBX2kC0V+8NYHfCG/iXs39DhUuYhF/bhcFoViGY/7WjxkcwVW9yzewb+PKlVsBpN5+pLZ60baOfqv3B6czFOxq4O7OeShK+qjNW7hjYLPCz4fbGiJ8K2dO2kLLd6iZItNgl3U3L1P7OSfvvMCMD1Sz07lSE9mefSrn6FSsTFNg3dOXaRUrtAanT7F2+Uz8bhMXjp8mjvWdOG25K3pBLfL4pEHNvHr5w/i9bhwuyxSmTxN8SBbNy5c62S+VGEgmZt1iqRvIsfwVJ7rc1spaAt76Yz66F0ZuzKv7b8yz+2jI+rD6zI5nRzjbz58h12+IG7TRGvNYC7Fry4c5V9svWPBfh+nyadH1FzPpk7+2Z8/ypvPHmCkb5ym9hgda1r43Q/eIpcpsGJdG31xg0DQU/U8t2VRquSYyhVIhOSt6ZTbtnQRDWvee/9dUukMO7euZOf2O/DPY82XbLF8ZWpk9uAeTRWqHm8aivbIdHDfsyYxc0Cy68oByraIF7c197GgdwYv4jUt3Ob0mjFKKdp9QU6Mj5Is5Ih6FuZYhdPk0yMWRM+mTnquLNP62jP72fv8IeKtEUJRPyOXxri0bwzf/SsJrrj29b5UqWAYiqDX83GbvUFmMkO5VCHcFJK5+Rqxy5doC/8nnnwwBxigP8DQJ9D66yg1e7hP5UvXpkc+0sPdN5FjPFOserzLVHRGp+eyd29oqe4siftpDXmwatA7nykXcRlm1X1KKVCKQqX8Mc9a/iTYxYLKZwq8/+oxWjpjmNb0ByyaCNGcydN/apRAPEDE76VYrjAyleaRrWvxuuZ+W6aTGZ7/h1c5e/ACGk1Te4w939hNx5q2hf6VyOWLaM28RrBXFfMlTMuYeW2cprWmnPsxYGCYXcB0R8nY1CmGR/cylFk708t9bdpk9h7uzitBvaUjcl1HyfSIuznoWZTe7u1NbRwfv0zE7Zn5w58uFQm73SS8Cz/Hbts2hVwRt9e9qCd5SbA3KG1nKBcPYJfPYphtmO5eDLP264qkJrNorW8Irng0QCLkwQ75uXR5Er/HxRM7N3LP+pVz1641P/nrZzk6PIKvJ0RcW2THcvzk/3yGb/5vXyMUW5irLU2lcvz2lSOcPj8KwMoVTTz+0Bbi0Y8PiGK5zFsnLrD31CUqts3OVR08sGk1mdEUL//kbfpODeFyW9z2wEbu+cLtuD2L34lxfQ933/goFwYtBlLr6Z90MTDpon/SRaa4AZgC3gfA7zZnRtjX5riv9XIngrXv4f40drZ0sn9kgDNT4/hNi4JtYwDf3NKLaSxs0J44cJ7Xf7mfyfEMXr+bux7bxq7PbsJY4P2CBHtD0naSQuo72PYYSvmxSx9SLryKJ/gvMaza9pmHYwEMw6BcqmC5roV7LlPgjrvXcd8jOylVKpjKuOkR3AdHz/KL7ABmhwelsmgUK1rcdJ4tcuK90/Q+tqOmvwNMt9P98Jf7SE5maUmEUAoGh5N8/+fv8vSf3F/VQXKV1pqfvHWIo31DJMJBPMrkzRMXOHZ+CPO1CxgoWlY0USlX2PfCIdKTWZ785u6a136thzs762j7xh7udYS9FTrCJVZES9zdk6UjPElnLExP2x/QFfMR9S/dHu7reS2Lp7fdyeGxIU4lx4i6vexs6aDFv7CXWrxwYpBn/u4VookQrSviFPMlXvnpuyhDcftnNy/ovkGCvSGV8q+i7YmZr9oA2h6nlPs57uD/XNMPrMc3PVJ57Zn9RBMh3B4Xk2Mp3F4X2+9ZB4DLvPlpCFtrfnriOEpBDGtmueGLVhFfUDE1dmvL1Gqt6c8mOZO6jNd0sTHSSsR94wG1S/3jjI2naWu5duZlPBpgcGSKcxcvs3HtjVNAgxMpjveP0BmPzLymHbEwBw+do6lQYFNXCwCWy6J5RYIT+85y3x/2Ek3c2tmdc/VwDyTzFCvVJ9/EA266Yj7Wt97Yw93i+hEB6yyG2XrlNaqgK324At/AdEdmK2FJc5smu1o62dWyeAui7X3+QwJhH76gd7oGr4t4W5S9zx9ix/0bF3xaRoK9AdmlQ2B8ZNpFxbDLF4AC4K3p/u56bBvBiJ/3XjpCKpll7fZuPrPnNsLxWx81jWWzZC2Nt6KwbY1hKBQKj1b0uUqs2NBx09vSWvPrS4d5c+TMdPBqsAyDf77mTjZEqtdFSWcLoCBHiUvGOGMqQ0B78JsuUun87LWms0wfp6v+Q1nMlSh5q+8zDIVhGKQnsjcEe7FsMzSZnx5xXxfcc/Vwd0Z9bO2MsGdre1VXSWfMh3+Wbxgzr4v9FUrpv8eu9KM0aKUxPbsxXMvzrFMnTIxM4QtUNwF4vC6Sl6coFcqY/oW9qlRNgl0ptQf4fwAT+Dut9f9ei+2KBaKCYE+Cuj7Ay0y/HWp/EE8pxda717L17rXz3xZguS0617Vz6Xg/Lq8LwzDIFAusaY2xalv3TW/rXHqM14fP0OEPY15ZRiFbLvKjc/v5q22P4TYtdGUYXdxPd3SAeCLNG5afAuDBYoo8mUiBPf4ts24/6veiuXGFS1/Iizl47SzOkobxks0lw8WLA2lGzx6/7gScHENTefRHerhbQ146Yz5uXxm7Mrftnzk4ebWH+9NSRgxX6N+iKxfQOoNhtKNM505OWo661rRy5vAl4q3XvuFkUzliiTAe38IfR5l3sCulTOA7wKNAH/CeUuoZrfXR+W5bLAzLcz/F7D+C9qOUhdY2ujKE5f0sSi3t06ib/H46w2FGLYvNsSBD50colcs090T5Hx55EOsmOmquOjoxiNswZ0IdwG+5mczm6csm6fFOQPY/AxDyefD0jOEZ82Gm16I16IImEQ2yr3iB+/W6G0bmXU0ReppjnBocx+vxkyrY06e2qxjnIh5eHdekLYv01csiBDz88rkTNevhng+lTJS1ekH3Uc/ufGwbZw5fYnw4STASIJfJk8sU+eLTDy3KsYlajNjvBE5rrc8CKKV+BPwhIMG+RJnuXbjsy5TzL02PKLExPb24fHsWrYZ0Js+li+OgoHtFE4HAzfWuK6V4avt2/n7/fiYCFSKbp+e27125ku1dNz8NA2Aaxqxr0ms0BhpyPwOCYExPGV32uFkZG2eCJFP5VrraY7QmwlxKpzjQN8Z4qnLDOtyXxrNMZCtAamb7lqHpiIaJl0u0pLIk3Aa3benkrt4eVjQFatbDLZzT3BHja3/5BO+9eJi+M8O09zRz5yNb6ZrlWMxCqEWwdwKXrrvdB9xVg+2KBaKUgcu3B8tzL9oeAxVekFbHj3P0aD/PPncQ254+oGeaJl94cicb1t/cm74lGOR/ue8+zo6PkymV6AqHaQlWz9eXSmU+fP8Ch9+/gNaabbtWsv32HlzXjei3xTr4/dApSnZl5iSWyWKOiNtLl09BJolW7UzmDPqSFkNDFn3JGMW8n1SuhUNnbSbTeQpFF//E3pnteiyDtqibaLDCjrWwLtHKxkQrXTE/K5sCi9bDLZyVaI/y+J/e58i+axHss71DbxgGKaWeBp4G6O6++XlQsXCUEUIZoUXd59RUjmefO0gk4sN95QBevlDiV78+QNe3dt/0yN1lmmxobp71Z1prnv3pPk4cHSAa9YOCl37zIefPjPClp+6e6SPuCsT4fOcWfnH2KOksZDKKUs6iy5Xg6f2n6BvfSf+kj0yxevTssmzCQZtwQBGKlIjnLEJFRdhSRNwGW3d6mGo/iWW4UCiK9nniwXX0Nj847yWRhbgZtQj2PuD65ucuYOCjD9Jafxf4LkxfaKMG+xXL0IWLl7Er9kyoA3g9LpITWS5eHGPTplubTpnNQN84p44N0t4RRQPpiqYYD/Hb46Oc/OUhprRBf/L6Hu7rPwaa497LdMb8rIy7uad7kM6Ym85Ihc5wlsv2EG9kN1NS098QvKdc6CK0d4dRSpErFnj21YN85tEEic7p1S211pxJn2Z9aD0dvsa+BqlYHLUI9veAdUqpVUA/8FXgazXYrqgjqVKed0cu8srhY/QPX6bk0rRHwzP9vEpN96jfqtl6uA+dHuFY0SLbl2eyrLl23RsL9vYR87vojM29Dre2s+jcf4PSUcAEZYD389xn3cVUKYdRVvynt9+gOXHtKkYlVcDyQP+pIm1Xgl0phakM+nP9EuxiUcw72LXWZaXUvwJ+x3Sv3Pe01kfmXZlY0ioVG6XUTc0VjxeyfOfw6xw7OExmoEB2NE3/VIrOcJQ7N3RjXunh7l7RdMNzP66H++pBytl6uOM+CxfQ4THYHFBELUXMMqhMpPjaH9/B1q1dN+xnNsrwowJ/hrbHwU6D2YxSPjxAsxkiOZUFTdUp4oYyMCxFIVd9QpBG4zFufnEzIeajJn3sWuvfAL+pxbbE0pbOFHjlnRMcPjkIwPaNnXz2rnUE/B8fWq8OnqKvL0l53CbeEsCPyeT5FMPjk7x7+AJtHS1s3LWaXx8bmQ7sOXq428JeumIfXYfby1hlkkNTl0gVshRfTtJS9tGVmD4oPDGeJtQcZMOGW+9KUEb8xhO6gHDQRyTkJZMtzPz+ATOAzluE1l/7nlCoFFBKsSqw6pb3vdhKdoXhbBq3YdLsq/31VMXikDNPxU0rV2x++Kv3GB1Pk4gHQcMHx/oYGp3i639096ynSWeLZd68MMTAeUW27Gdo3CKPn1x7E7mS4igmJIGXzwLV63BPX2fSf1M93C9dOs3vz50g4fXTGgwzei+cfW8UPaDxWS5Wr29j9+PbcLks8pUyR8eGGcmlafOH2BRvwWPe+kfBMBSPP7SVH/96H9lsEZfbIpcrsq1zPS0bxhkvjgEKt+Hi4eZHCbmW9oWgj4+P8MOTH5ItFdFAdyjKP9+4g7jXP+dzxdIiwS5u2vm+MUYup2hruXY2XTQW4kj/JD988wxl05oZad+4Dvf0CogKjdfSeFwVYu4SnR4/X3lgE2tbwnTGfLSFvbfcw12olHnp0mna/aGZCyq0JaLwoMm6aDt3WC1gg6EUyUKOvz28l9FcBsswKGubVl+Qb229i4j71pdSWN2d4F88dR8Hj/aRnMyyakWCzevbcbtNJooTlHWZJncTlrG0P2qXcxm+d3Q/YbeHjmB4+kpDmRT/+eh+/u3O+zBk5L6sLO13m3CU1prJXGlmRcB3jg3wXtKmlEozWbSZLNrkr1wg+Ge/OQlM93BfPRh5dR3uipXj9+eOURzM0hR1owxIFQs0E+Qzq5v5k7vnN0UxVSxQsu2ZUL/KVVb89vcf0Hfex/RSMJpL95XpDybxWhYxFaHT28poLs2Ll07zR2s+3VooiViQh+/deMP9TZ4bjxksVR+MDqK1JuCaXsNEKUWLP0B/eoq+9CTdoajDFYpbIcHewK5fh/ujlyq7ejtTrFQ9x6Ug6qkQdZusCFhE3ApdKPDHD2+hd30bTYEb1+HWWrOpx8U/vLyPsaEMGmj2BVnf1swX7pt9nZVbEXJ5sAyDYqVyXbhrTpzqp2lK0doZA+BIoJ/3sxdp98QxLZPR0hhT5RTr/WvYP9r/qYO9HqRLxVnXJ1co8uX6vdJQvZJgr2PX1uGePbRvXIcbwl6Lrpif7iY/96xtmrk4cFfMT1vYw7PPH+DSYJJ4dHraYmwizap1CR7b0f2xHTJKKR5dsZF7vraKE4Mj5DNlmoNBVrXHsWpw5SCvZfFw11qePX+MJm8An+ViaHKSbCrPQ77pC3fkzRKD4Umssot8ukTQ78evfGQqOcYKk4SspT3/vdDWxxK81n+uasGyUqWCUtARXNyT2MT8SbAvYxVbMzSVvxbUVRdRyH7sOtyd0WvrcHfGfKyI+acvY3ZdD/fH+e+e7OWdA+f44GgfKHjgznXcvaPnptoeAy4Pu7preyGPq3avWEPA5eblvtMMZlN0ekMELkcJt0xPLWTNIoZWxMpuUva1EaipDIaK43yuq3FH6wDrowm2NLVyeGyYoMtFybYpVSp8cc0Wgi5p01xulP4UJ4XMV29vr963b9+i73e5KVVsBpN5+q6su30tuKdvD03mKX/MOtxXT7bpivmvdJRML+ca8NT/33KtNbat+d7/8Sy2bRMI+ciaRd5qPo09XoE1fvKWBgV5O88dsQ38r7d97lN1xtSTkl3hyNgwH14ewm+5uL2lk1WRxVtDSMxNKbVfa9071+Ma+53ssHypwkAyN+sUSd9EjuGpPPYsPdzV15n0zwT5fNfh/jQm8jnOpyZwGSZrI3G8lvPL/iqlME3F40/dzc/+7vdkUnkMw8BXMSl1G+xYsZKCXWG4MEnIauEvNu5u+FAHcBkmO5o72NE8/2UdhLNkxL6AssXylUuUzR7co6lC1eOv7+HuujI9stjrcN+KV/vP8uz5Y9MnECnwmS6+ufkOesIxp0ubkUpmOXX4EpmpPK2r41wMj7M/eYaSXWFtsJ1H2reT8DT2/LpYPm52xC7BPg9T+dL0Jco+enDyhh7uaS5T0Xnl0mRZHIl1AAAOgklEQVRdUX/1ld3j/mW1Dvel9CT/8eAbtPqCM0veThULaDTf7n1o5r6lyNY2ttZYS7hGIWYjUzHz9NEe7tm6Sqby1W1g1/dwb+2MXNdRMj3irqd1uA+PDWEqVRXgYbeHgcwkF1NJ1kSWbg+3oQzq5J9BiFk1bLB/mh7ugNucmSLp7YndMGUyWw93vbK1DfrG31WjcOJboBDimroN9ms93NmZUfe14L6VHu5rFwmO+FwNE9xz2Rxv5eW+s1Rse+bElmy5iNe0WCFnKQrhqGUb7LOtwz19oHK6n/vjeri7YtM93Ls3tlSNuG+mh1tc0xOKsbtrNS/3nZ25z2Ua/NmG26XDRAiHLdtP4L/+0QF+c2io6r7mkIeumI+tnRH2bG2v6irpjPnwu5ftr7vkKKX4/MqN7Gzu5MzkGG7DZGO85VMtpCWEqK1lm3Rf6V3B/euaZ7pKnOjhbnRKKToCYToC0i4oxFKybIP9oQ0tTpcghBBL0vJomhZCCHHTlu2IXQixdGmtOZ+a4OzUOH7LxZZ4K2E5/rJoJNiFEDVla80/nf2QvcMXMdT0eQ2/umDxjQ13sDaacLq8hiBTMUKImjqeHOGd4Yt0BiJ0BiJ0BaP4LRffP32Asm3PvQExbxLsQoia+nBsEJ9pVV0nNejykCkVGMhMOlhZ45BgF0LUlNswmW1crplep0csPHmVhRA1tSPRQbFSrpp2Gc9nafYG5ZyHRSIHT8WCGB+ZYmIsTSjip7k9ImvsNJBVoTif797Iby+dmL5DQ9jj5U837KqanhELR4Jd1FS5VOGFn+/n6IELKEOhbc2q9W088dTdeGQtnoaglOLhrnXsau7kUnoSj2mxOhxf0mv01xsJdlFTB/ee4fD+c7R2xjGM6Va3cyeHePOFw+z+wk6nyxOLKObxE/P4nS6jIckcu7gpZbvIVHGQXDn5iY/74J0zxJpCMxcUUUrR1Brm0HvnqFSk1U2IxSAjdjGn/uyHnJx8EVuX0UqT8Kxhc/QJ3IbvhseWimXcnuopF8NQVMo2yAU4hFgUMmIXn2iieImjyd/gNoIEXS0EzRbG8mc5lnxu1sdvuq2b5Fiq6r7k5TRrNnVgWjLHuhCSxUnOps/RnxuYvrKVaHgyYhefqD9zEEu5sQw3MD21ErASjOZPka+k8Jqhqsf3PriBcyeHGO6fwHKZlEsVgmEfD35+uxPl1zVb27wz9h6HJo+gUIAm6oryePujhFyhOZ8v6pcEu/hERTuNoaqnVpQyUEpRtgvwkWAPBL187c8f5uyxAUYGksSbQ6zd3InX717MshvCufQFDiYP0expmjnxZ6I4ye9H3+DJjscdrk44SYJdfKJm7zrGC+erRuYlO4fL8OO3YrM+x+222HhbNxtv616sMhvSidQp/Kav6mzOqCvMQG6QTDlDwAo4WJ1wkgS7+ERtvi0MZg8zWRrAZfip6BJom+2xL2EomTN3UkWXP/aEH5lrb2wS7OITuQwvu5q+ynDuOJcLZ/GZEdr9Wwm55ApWTlsXWsMrI6/jN/0zZ/amyxni7jhBK+hwdcJJEuxiTpbhoTNwG52B25wuRVxnbXAN57MXuZC5iIGBRuMxPXy25X5ZwqHBSbALsUxZhsVjrQ8zmBtipDCKz/TRE+jGa8qVihqdBLsQy5ihDDr9HXT6O5wuRSwh8zpBSSn1FaXUEaWUrZTqrVVRQgghPr35nnl6GPgy8FoNahFCCFED85qK0VofA+RAjRBCLCGyVowQQtSZOUfsSqkXgbZZfvRtrfUvb3ZHSqmngacBurvljEQhxOIqlSu8f7afA+cGsAxF79oVbFvZhmnU3/h2zmDXWj9Six1prb8LfBegt7dX1m8VQtyy/oEJ3v/gAsnJHKtWJdixrZtgwDPn8yq2zQ9eP8CJgVGifh+21vz4zYOcH53gi3duWYTKF1f9/akSQtSl4yeH+K8/fJuTp4eZnMryxpun+K8/eIt0pjDnc88Nj3Ny4DKd8QhBn4ew30tnPMK+032MTKYXofrFNd92xy8ppfqAzwDPKqV+V5uyhBDimnLF5sVXjhCJeInHAwQCHlpbw0xN5fjgw4tzPn9gfArDMKoaPa5e5Ws4KcFeRWv9c611l9bao7Vu1Vp/rlaFCSHEVelUnkymgM9bvfxzMOThzNmROZ8f9nvQs1zBSykIeutvSWmZihFCLHker4VS6obr5hYKZaLRuS+Yvb6jhZDXzXgqi9YaW2tGJtO0hIN0N0cXqmzHSLALIZY8n9fN9q1djI6mZsI9ny9RLJS5fcfKOZ/v97j4+u47aI4EGUqmGJ5Isbo1zn//2dsbsytGCCGWgt0PbkIDhw73oQGvx8UfPLGDrs74TT2/NRrk6UfvJJUvYChF0Dt3N81yJcEuhFgW3G6Lxx/dxoP3bSCfLxEO+7DMWxttK6UI++p/9UsJdiHEsuL3ufH76u+AZy3V3+SSEEI0OAl2IYSoM8t2KiaXLVAp2wRCXlldcpGNjaV5+53TXLhwmXgswF13rWH1arkGqhBLxbIL9kwqz4u/OsDpY4MAtHZEeeyLu2hpr79e1KVofDzDP/7jG5QrmnDYy+jlFD/+8V6+8IWdbN3a5XR5QgiW2VSMbdv84gdvc+bEIInWMM1tYZLjaX7yvdfIpPNOl9cQ9u07S7lsk0gEcbstwmEfsXiAV189fsPJI0IIZyyrYB8eSDJ4aYzm1giGoVBKEYkFKORLnD464HR5DeHipXECwer+X6/XRTZXIJstOlSVEOJ6yyrYs+kCSt1YsmEYTCYzDlTUeBKJEPl8qeq+UqmCZZl4vS6HqhJCXG9ZBXtTS3h6nYfrvvJrrSmXK3SsaHKwssZxxx2rKBbLpNMFtNYUi2VGR1PcfdcaXC7T6fKEECyzYI/GA9x+z1qGB5KkJnNk03mG+5OsWJWgZ22r0+U1hM6OGF/5Z3fi9ViMjKTIZYvs3r2Zu+5a43RpQogrll1XzIN7ttHeFefgu2cpFsv03ruebb09WDJaXDSrV7ewalUzhUIZl8vEvMXTuoUQC2vZBbthGGzcvoKN21c4XUpDU0rJnLoQS5QMtYQQos5IsAshRJ2RYBdCiDojwS6EEHVGgl0IIeqMBLsQQtQZCXYhhKgzEuxCCFFnJNiFEKLOSLALIUSdkWAXQog6I8EuhBB1RoJdCCHqjAS7EELUGQl2IYSoMxLsQghRZyTYhRCizkiwCyFEnZFgF0KIOiPBLoQQdUaCXQgh6owEuxBC1Jl5BbtS6j8opY4rpT5USv1cKRWtVWFCCCE+nfmO2F8AtmqttwMngb+af0lCCCHmY17BrrV+XmtdvnLzHaBr/iUJIYSYj1rOsX8DeK6G2xNCCPEpWHM9QCn1ItA2y4++rbX+5ZXHfBsoA9//hO08DTwN0N3d/amKFUIIMbc5g11r/cgn/Vwp9WfAk8DDWmv9Cdv5LvBdgN7e3o99nBBCiPmZM9g/iVJqD/DvgAe11tnalCSEEGI+5jvH/tdACHhBKfWBUupva1CTEEKIeZjXiF1rvbZWhQghhKgNOfNUCCHqjAS7EELUGQl2IYSoMxLsQghRZyTYhRCizkiwCyFEnZFgF0KIOiPBLoQQdUaCXQgh6owEuxBC1BkJdiGEqDMS7EIIUWck2IUQos5IsAshRJ2RYBdCiDojwS6EEHVGgl0IIeqMBLsQQtSZhg32sl0hXcpS0RWnSxFCiJqa1zVPlyOtNQcmjrF3/BDFSgmP6eaeph1si65DKeV0eUIIMW8NN2I/MnWaV0bew2d6afbG8ZoeXhh5h5Op806XJoQQNdFwwb537BBRdxi34QLAbbiIWAH2jh92uDIhhKiNhgp2rTWpUhav4a6632N6mCqmHapKCCFqq6GCXSlFh6+ZVDlTdX+qlKHL3+pQVUIIUVsNFewA9zXvomiXGC9Okq8UGC9MUtEVPpO4zenShBCiJhquK6bD18xXux9n//hRRgvjrAut5Pb4JhKemNOlCSFETTRcsAO0eOM83nGf02UIIcSCaLipGCGEqHcS7EIIUWck2IUQos5IsAshRJ2RYBdCiDojwS6EEHVGaa0Xf6dKjQIXFn3HNycBXHa6iCVCXotq8npUk9ej2mK8Hiu11s1zPciRYF/KlFL7tNa9TtexFMhrUU1ej2ryelRbSq+HTMUIIUSdkWAXQog6I8F+o+86XcASIq9FNXk9qsnrUW3JvB4yxy6EEHVGRuxCCFFnJNg/Qin1H5RSx5VSHyqlfq6Uijpdk5OUUl9RSh1RStlKqSVxxN8JSqk9SqkTSqnTSql/73Q9TlJKfU8pNaKUavjrSSqlViilXlFKHbvyOfk3TtcEEuyzeQHYqrXeDpwE/srhepx2GPgy8JrThThFKWUC3wEeBzYDTymlNjtblaP+C7DH6SKWiDLwl1rrTcDdwJ8vhfeGBPtHaK2f11qXr9x8B+hysh6naa2Paa1POF2Hw+4ETmutz2qti8CPgD90uCbHaK1fA8adrmMp0FoPaq3fv/L/U8AxoNPZqiTY5/IN4DmnixCO6wQuXXe7jyXw4RVLi1KqB9gJ7HW2kga9gpJS6kWgbZYffVtr/csrj/k201+zvr+YtTnhZl6PBqdmuU/aycQMpVQQ+CnwF1rrKafrachg11o/8kk/V0r9GfAk8LBugH7QuV4PQR+w4rrbXcCAQ7WIJUYp5WI61L+vtf6Z0/WATMXcQCm1B/h3wB9orbNO1yOWhPeAdUqpVUopN/BV4BmHaxJLgFJKAX8PHNNa/19O13OVBPuN/hoIAS8opT5QSv2t0wU5SSn1JaVUH/AZ4Fml1O+crmmxXTmY/q+A3zF9cOwnWusjzlblHKXUD4G3gQ1KqT6l1DedrslB9wJ/Cuy+khcfKKU+73RRcuapEELUGRmxCyFEnZFgF0KIOiPBLoQQdUaCXQgh6owEuxBC1BkJdiGEqDMS7EIIUWck2IUQos78/xUTVvfB3UVWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots best-fit line via polyfit\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "\n",
    "# Plots the random x and y data points we created\n",
    "# Interestingly, alpha makes it more aesthetically pleasing\n",
    "plt.scatter(x, y, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Aim of Linear Regression\n",
    "- Minimize the distance between the points and the line ($y = \\alpha x + \\beta$)\n",
    "- Adjusting\n",
    "    - Coefficient: $\\alpha$\n",
    "    - Bias/intercept: $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Linear Regression Model with PyTorch\n",
    "\n",
    "### 2.1 Example\n",
    "- Coefficient: $\\alpha = 2$\n",
    "- Bias/intercept: $\\beta = 1$\n",
    "- Equation: $y = 2x + 1$\n",
    "\n",
    "### 2.2 Building a Toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a list of values from 0 to 11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [i for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert list of numbers to numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to numpy\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to 2-dimensional array**\n",
    "\n",
    "If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important : 2-Dimension is required\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create list of y values**\n",
    "\n",
    "We want y values for every x value we have above. \n",
    "    \n",
    "$y = 2x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = [2*i+1 for i in x_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to numpy array**\n",
    "\n",
    "You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshape y numpy array to 2-dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critical Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Model**\n",
    "\n",
    "1. Linear model\n",
    "    - True Equation: $y = 2x + 1$\n",
    "2. Forward\n",
    "    - Example\n",
    "        - Input $x = 1$\n",
    "        - Output $\\hat y = ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class\n",
    "class LinearRegressionModel (nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate Model Class**\n",
    "\n",
    "- input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate Loss Class**\n",
    "- MSE Loss: Mean Squared Error\n",
    "- $MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)$\n",
    "    - $\\hat y$: prediction\n",
    "    - $y$: true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # Mean Squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate Optimizer Class**\n",
    "\n",
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$\n",
    "        - $\\theta$: parameters (our variables)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "        - parameters: $\\alpha$ and $\\beta$ in $y = \\alpha x + \\beta$\n",
    "        - desired parameters: $\\alpha = 2$ and $\\beta = 1$ in $ y = 2x + 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**\n",
    "\n",
    "- 1 epoch: going through the whole x_train data once\n",
    "    - 100 epochs: \n",
    "        - 100x mapping `x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`\n",
    "            \n",
    "- Process \n",
    "    1. Convert inputs/labels to tensors with gradients\n",
    "    2. Clear gradient buffets\n",
    "    3. Get output given inputs \n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. parameters\n",
    "    6. Update parameters using gradients\n",
    "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 142.97166442871094\n",
      "epoch 2, loss 11.741623878479004\n",
      "epoch 3, loss 1.0367070436477661\n",
      "epoch 4, loss 0.16265855729579926\n",
      "epoch 5, loss 0.09049323946237564\n",
      "epoch 6, loss 0.08374466747045517\n",
      "epoch 7, loss 0.08234144747257233\n",
      "epoch 8, loss 0.08138369023799896\n",
      "epoch 9, loss 0.08047176152467728\n",
      "epoch 10, loss 0.07957299798727036\n",
      "epoch 11, loss 0.07868427038192749\n",
      "epoch 12, loss 0.07780573517084122\n",
      "epoch 13, loss 0.07693682610988617\n",
      "epoch 14, loss 0.07607768476009369\n",
      "epoch 15, loss 0.07522815465927124\n",
      "epoch 16, loss 0.07438808679580688\n",
      "epoch 17, loss 0.07355744391679764\n",
      "epoch 18, loss 0.07273602485656738\n",
      "epoch 19, loss 0.07192382216453552\n",
      "epoch 20, loss 0.07112062722444534\n",
      "epoch 21, loss 0.07032637298107147\n",
      "epoch 22, loss 0.06954120099544525\n",
      "epoch 23, loss 0.06876461207866669\n",
      "epoch 24, loss 0.06799671798944473\n",
      "epoch 25, loss 0.06723742932081223\n",
      "epoch 26, loss 0.066486656665802\n",
      "epoch 27, loss 0.06574422121047974\n",
      "epoch 28, loss 0.06500998884439468\n",
      "epoch 29, loss 0.06428395956754684\n",
      "epoch 30, loss 0.06356614083051682\n",
      "epoch 31, loss 0.06285637617111206\n",
      "epoch 32, loss 0.06215430423617363\n",
      "epoch 33, loss 0.061460401862859726\n",
      "epoch 34, loss 0.06077399477362633\n",
      "epoch 35, loss 0.060095395892858505\n",
      "epoch 36, loss 0.05942424759268761\n",
      "epoch 37, loss 0.058760832995176315\n",
      "epoch 38, loss 0.058104634284973145\n",
      "epoch 39, loss 0.05745558813214302\n",
      "epoch 40, loss 0.05681411549448967\n",
      "epoch 41, loss 0.05617968365550041\n",
      "epoch 42, loss 0.05555226653814316\n",
      "epoch 43, loss 0.05493202060461044\n",
      "epoch 44, loss 0.05431864410638809\n",
      "epoch 45, loss 0.0537119060754776\n",
      "epoch 46, loss 0.05311219394207001\n",
      "epoch 47, loss 0.05251912772655487\n",
      "epoch 48, loss 0.05193266272544861\n",
      "epoch 49, loss 0.05135275796055794\n",
      "epoch 50, loss 0.05077926442027092\n",
      "epoch 51, loss 0.05021224543452263\n",
      "epoch 52, loss 0.049651507288217545\n",
      "epoch 53, loss 0.04909711703658104\n",
      "epoch 54, loss 0.04854869097471237\n",
      "epoch 55, loss 0.04800659790635109\n",
      "epoch 56, loss 0.04747060686349869\n",
      "epoch 57, loss 0.04694052040576935\n",
      "epoch 58, loss 0.046416327357292175\n",
      "epoch 59, loss 0.045897964388132095\n",
      "epoch 60, loss 0.045385368168354034\n",
      "epoch 61, loss 0.04487856477499008\n",
      "epoch 62, loss 0.044377539306879044\n",
      "epoch 63, loss 0.0438818633556366\n",
      "epoch 64, loss 0.043391939252614975\n",
      "epoch 65, loss 0.04290741682052612\n",
      "epoch 66, loss 0.042428236454725266\n",
      "epoch 67, loss 0.041954413056373596\n",
      "epoch 68, loss 0.04148593917489052\n",
      "epoch 69, loss 0.041022736579179764\n",
      "epoch 70, loss 0.04056454077363014\n",
      "epoch 71, loss 0.04011158272624016\n",
      "epoch 72, loss 0.03966366499662399\n",
      "epoch 73, loss 0.039220694452524185\n",
      "epoch 74, loss 0.03878280147910118\n",
      "epoch 75, loss 0.03834961727261543\n",
      "epoch 76, loss 0.0379214733839035\n",
      "epoch 77, loss 0.03749798610806465\n",
      "epoch 78, loss 0.03707929700613022\n",
      "epoch 79, loss 0.03666521608829498\n",
      "epoch 80, loss 0.03625572472810745\n",
      "epoch 81, loss 0.03585093840956688\n",
      "epoch 82, loss 0.0354505330324173\n",
      "epoch 83, loss 0.035054679960012436\n",
      "epoch 84, loss 0.034663207828998566\n",
      "epoch 85, loss 0.0342760905623436\n",
      "epoch 86, loss 0.0338933952152729\n",
      "epoch 87, loss 0.03351493179798126\n",
      "epoch 88, loss 0.03314060717821121\n",
      "epoch 89, loss 0.03277057781815529\n",
      "epoch 90, loss 0.03240462765097618\n",
      "epoch 91, loss 0.032042860984802246\n",
      "epoch 92, loss 0.0316849909722805\n",
      "epoch 93, loss 0.031331147998571396\n",
      "epoch 94, loss 0.03098127990961075\n",
      "epoch 95, loss 0.030635295435786247\n",
      "epoch 96, loss 0.030293220654129982\n",
      "epoch 97, loss 0.029954949393868446\n",
      "epoch 98, loss 0.029620392248034477\n",
      "epoch 99, loss 0.029289590194821358\n",
      "epoch 100, loss 0.028962621465325356\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # 1) convert numpy array to torch variable\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    labels = torch.from_numpy(y_train)\n",
    "    \n",
    "    # 2) clear gradients with respect to parameters of model\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 3) forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 4) calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 5) getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # 6) updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at predicted values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purely inference\n",
    "predicted = model(torch.from_numpy(x_train)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68342334],\n",
       "       [ 2.7290132 ],\n",
       "       [ 4.7746034 ],\n",
       "       [ 6.8201933 ],\n",
       "       [ 8.865783  ],\n",
       "       [10.911372  ],\n",
       "       [12.956963  ],\n",
       "       [15.002553  ],\n",
       "       [17.048143  ],\n",
       "       [19.093733  ],\n",
       "       [21.139322  ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at training values**\n",
    "\n",
    "These are the true values, you can see how it's able to predict similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot of predicted and actual values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) clear figure\n",
    "plt.clf()\n",
    "\n",
    "# 2) get predictions\n",
    "predicted = model(torch.from_numpy(x_train)).data.numpy()\n",
    "\n",
    "# 3) plot true data\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "\n",
    "# 4) plot predictions\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.9)\n",
    "\n",
    "# 5) legend and plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[2.0456]])), ('linear.bias', tensor([0.6834]))])\n"
     ]
    }
   ],
   "source": [
    "save_model = False\n",
    "if save_model is True:\n",
    "    torch.save(model.state_dict(), 'awesome_model.pkl')\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model is True:\n",
    "    model.load_state_dict(torch.load('awesome_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Linear Regression Model with PyTorch (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CPU Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 279.8941650390625\n",
      "epoch 2, loss 22.88564109802246\n",
      "epoch 3, loss 1.9216471910476685\n",
      "epoch 4, loss 0.21106809377670288\n",
      "epoch 5, loss 0.0709347128868103\n",
      "epoch 6, loss 0.05890452116727829\n",
      "epoch 7, loss 0.057330235838890076\n",
      "epoch 8, loss 0.05661528930068016\n",
      "epoch 9, loss 0.05597678944468498\n",
      "epoch 10, loss 0.05535125732421875\n",
      "epoch 11, loss 0.054733145982027054\n",
      "epoch 12, loss 0.05412193015217781\n",
      "epoch 13, loss 0.053517647087574005\n",
      "epoch 14, loss 0.05292001739144325\n",
      "epoch 15, loss 0.05232904106378555\n",
      "epoch 16, loss 0.05174465477466583\n",
      "epoch 17, loss 0.051166828721761703\n",
      "epoch 18, loss 0.05059543251991272\n",
      "epoch 19, loss 0.050030581653118134\n",
      "epoch 20, loss 0.04947177320718765\n",
      "epoch 21, loss 0.04891926050186157\n",
      "epoch 22, loss 0.0483730286359787\n",
      "epoch 23, loss 0.047832902520895004\n",
      "epoch 24, loss 0.04729876294732094\n",
      "epoch 25, loss 0.04677053540945053\n",
      "epoch 26, loss 0.046248286962509155\n",
      "epoch 27, loss 0.04573185741901398\n",
      "epoch 28, loss 0.045221105217933655\n",
      "epoch 29, loss 0.04471616446971893\n",
      "epoch 30, loss 0.04421685263514519\n",
      "epoch 31, loss 0.04372313246130943\n",
      "epoch 32, loss 0.04323480278253555\n",
      "epoch 33, loss 0.04275200888514519\n",
      "epoch 34, loss 0.042274534702301025\n",
      "epoch 35, loss 0.041802506893873215\n",
      "epoch 36, loss 0.041335780173540115\n",
      "epoch 37, loss 0.04087422043085098\n",
      "epoch 38, loss 0.04041769728064537\n",
      "epoch 39, loss 0.03996635973453522\n",
      "epoch 40, loss 0.03952014818787575\n",
      "epoch 41, loss 0.0390787348151207\n",
      "epoch 42, loss 0.038642335683107376\n",
      "epoch 43, loss 0.038210779428482056\n",
      "epoch 44, loss 0.037784166634082794\n",
      "epoch 45, loss 0.03736225143074989\n",
      "epoch 46, loss 0.03694508224725723\n",
      "epoch 47, loss 0.03653242811560631\n",
      "epoch 48, loss 0.036124538630247116\n",
      "epoch 49, loss 0.03572116792201996\n",
      "epoch 50, loss 0.035322148352861404\n",
      "epoch 51, loss 0.034927792847156525\n",
      "epoch 52, loss 0.03453776240348816\n",
      "epoch 53, loss 0.03415203094482422\n",
      "epoch 54, loss 0.03377072140574455\n",
      "epoch 55, loss 0.03339352458715439\n",
      "epoch 56, loss 0.03302069380879402\n",
      "epoch 57, loss 0.03265203535556793\n",
      "epoch 58, loss 0.03228733688592911\n",
      "epoch 59, loss 0.031926851719617844\n",
      "epoch 60, loss 0.031570252031087875\n",
      "epoch 61, loss 0.031217748299241066\n",
      "epoch 62, loss 0.030869130045175552\n",
      "epoch 63, loss 0.030524449422955513\n",
      "epoch 64, loss 0.03018351085484028\n",
      "epoch 65, loss 0.029846476390957832\n",
      "epoch 66, loss 0.029513167217373848\n",
      "epoch 67, loss 0.029183587059378624\n",
      "epoch 68, loss 0.02885769121348858\n",
      "epoch 69, loss 0.028535418212413788\n",
      "epoch 70, loss 0.02821684256196022\n",
      "epoch 71, loss 0.02790166437625885\n",
      "epoch 72, loss 0.02759021334350109\n",
      "epoch 73, loss 0.027282051742076874\n",
      "epoch 74, loss 0.026977412402629852\n",
      "epoch 75, loss 0.026676127687096596\n",
      "epoch 76, loss 0.026378288865089417\n",
      "epoch 77, loss 0.026083771139383316\n",
      "epoch 78, loss 0.02579246088862419\n",
      "epoch 79, loss 0.02550434321165085\n",
      "epoch 80, loss 0.02521967515349388\n",
      "epoch 81, loss 0.024938007816672325\n",
      "epoch 82, loss 0.024659501388669014\n",
      "epoch 83, loss 0.024384090676903725\n",
      "epoch 84, loss 0.02411189116537571\n",
      "epoch 85, loss 0.023842554539442062\n",
      "epoch 86, loss 0.023576313629746437\n",
      "epoch 87, loss 0.023313092067837715\n",
      "epoch 88, loss 0.02305271103978157\n",
      "epoch 89, loss 0.022795312106609344\n",
      "epoch 90, loss 0.022540738806128502\n",
      "epoch 91, loss 0.02228899486362934\n",
      "epoch 92, loss 0.02204013429582119\n",
      "epoch 93, loss 0.021793970838189125\n",
      "epoch 94, loss 0.02155064046382904\n",
      "epoch 95, loss 0.02130994200706482\n",
      "epoch 96, loss 0.021072043105959892\n",
      "epoch 97, loss 0.020836738869547844\n",
      "epoch 98, loss 0.02060406468808651\n",
      "epoch 99, loss 0.020373934879899025\n",
      "epoch 100, loss 0.020146455615758896\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "STEP 1 : create model class\n",
    "'''\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return output\n",
    "    \n",
    "'''\n",
    "STEP 2 : instantiate model class\n",
    "'''\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 3 : instantiate loss class\n",
    "'''\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "'''\n",
    "STEP 4 : instantiate optimizer class\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 5 : train the model\n",
    "'''\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    \n",
    "    # 1) convert numpy to torch variables\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    labels = torch.from_numpy(y_train)\n",
    "    \n",
    "    # 2) clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 3) foward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 4) calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 5) getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # 6) updatino parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch {0}, loss {1}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU Summary**\n",
    "\n",
    "- Just remember always 2 things must be on GPU\n",
    "    - `model`\n",
    "    - `tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 161.7766571044922\n",
      "epoch 2, loss 13.696737289428711\n",
      "epoch 3, loss 1.6127339601516724\n",
      "epoch 4, loss 0.6215453147888184\n",
      "epoch 5, loss 0.5352253317832947\n",
      "epoch 6, loss 0.522773802280426\n",
      "epoch 7, loss 0.5164079666137695\n",
      "epoch 8, loss 0.5105983018875122\n",
      "epoch 9, loss 0.5048931837081909\n",
      "epoch 10, loss 0.49925491213798523\n",
      "epoch 11, loss 0.4936795234680176\n",
      "epoch 12, loss 0.48816683888435364\n",
      "epoch 13, loss 0.4827154278755188\n",
      "epoch 14, loss 0.47732487320899963\n",
      "epoch 15, loss 0.47199487686157227\n",
      "epoch 16, loss 0.4667241871356964\n",
      "epoch 17, loss 0.4615125358104706\n",
      "epoch 18, loss 0.45635873079299927\n",
      "epoch 19, loss 0.45126259326934814\n",
      "epoch 20, loss 0.4462234675884247\n",
      "epoch 21, loss 0.4412403404712677\n",
      "epoch 22, loss 0.4363134205341339\n",
      "epoch 23, loss 0.4314410388469696\n",
      "epoch 24, loss 0.42662313580513\n",
      "epoch 25, loss 0.4218592345714569\n",
      "epoch 26, loss 0.41714832186698914\n",
      "epoch 27, loss 0.41249018907546997\n",
      "epoch 28, loss 0.4078839421272278\n",
      "epoch 29, loss 0.40332919359207153\n",
      "epoch 30, loss 0.39882519841194153\n",
      "epoch 31, loss 0.3943716287612915\n",
      "epoch 32, loss 0.3899676501750946\n",
      "epoch 33, loss 0.38561293482780457\n",
      "epoch 34, loss 0.3813069462776184\n",
      "epoch 35, loss 0.3770488500595093\n",
      "epoch 36, loss 0.37283840775489807\n",
      "epoch 37, loss 0.36867496371269226\n",
      "epoch 38, loss 0.3645579516887665\n",
      "epoch 39, loss 0.3604871928691864\n",
      "epoch 40, loss 0.35646164417266846\n",
      "epoch 41, loss 0.3524811565876007\n",
      "epoch 42, loss 0.3485449552536011\n",
      "epoch 43, loss 0.34465277194976807\n",
      "epoch 44, loss 0.34080418944358826\n",
      "epoch 45, loss 0.33699849247932434\n",
      "epoch 46, loss 0.33323538303375244\n",
      "epoch 47, loss 0.32951420545578003\n",
      "epoch 48, loss 0.32583439350128174\n",
      "epoch 49, loss 0.3221959173679352\n",
      "epoch 50, loss 0.3185979425907135\n",
      "epoch 51, loss 0.3150402307510376\n",
      "epoch 52, loss 0.31152233481407166\n",
      "epoch 53, loss 0.308043509721756\n",
      "epoch 54, loss 0.3046036660671234\n",
      "epoch 55, loss 0.30120226740837097\n",
      "epoch 56, loss 0.29783865809440613\n",
      "epoch 57, loss 0.2945128083229065\n",
      "epoch 58, loss 0.29122394323349\n",
      "epoch 59, loss 0.2879718840122223\n",
      "epoch 60, loss 0.28475621342658997\n",
      "epoch 61, loss 0.28157636523246765\n",
      "epoch 62, loss 0.2784319818019867\n",
      "epoch 63, loss 0.2753227949142456\n",
      "epoch 64, loss 0.27224844694137573\n",
      "epoch 65, loss 0.26920825242996216\n",
      "epoch 66, loss 0.266201913356781\n",
      "epoch 67, loss 0.26322948932647705\n",
      "epoch 68, loss 0.2602900266647339\n",
      "epoch 69, loss 0.2573833167552948\n",
      "epoch 70, loss 0.25450918078422546\n",
      "epoch 71, loss 0.2516670227050781\n",
      "epoch 72, loss 0.24885667860507965\n",
      "epoch 73, loss 0.24607768654823303\n",
      "epoch 74, loss 0.2433299422264099\n",
      "epoch 75, loss 0.24061259627342224\n",
      "epoch 76, loss 0.23792578279972076\n",
      "epoch 77, loss 0.23526883125305176\n",
      "epoch 78, loss 0.2326417863368988\n",
      "epoch 79, loss 0.2300439327955246\n",
      "epoch 80, loss 0.22747498750686646\n",
      "epoch 81, loss 0.22493483126163483\n",
      "epoch 82, loss 0.22242306172847748\n",
      "epoch 83, loss 0.219939187169075\n",
      "epoch 84, loss 0.2174830585718155\n",
      "epoch 85, loss 0.21505451202392578\n",
      "epoch 86, loss 0.21265310049057007\n",
      "epoch 87, loss 0.2102784961462021\n",
      "epoch 88, loss 0.207930326461792\n",
      "epoch 89, loss 0.20560850203037262\n",
      "epoch 90, loss 0.20331233739852905\n",
      "epoch 91, loss 0.2010420262813568\n",
      "epoch 92, loss 0.19879692792892456\n",
      "epoch 93, loss 0.1965770125389099\n",
      "epoch 94, loss 0.19438184797763824\n",
      "epoch 95, loss 0.19221128523349762\n",
      "epoch 96, loss 0.19006481766700745\n",
      "epoch 97, loss 0.1879424899816513\n",
      "epoch 98, loss 0.18584385514259338\n",
      "epoch 99, loss 0.18376842141151428\n",
      "epoch 100, loss 0.18171626329421997\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "STEP 1 : create model class\n",
    "'''\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return output\n",
    "    \n",
    "'''\n",
    "STEP 2 : instantiate model class\n",
    "'''\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "'''\n",
    "STEP 3 : instantiate loss class\n",
    "'''\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "'''\n",
    "STEP 4 : instantiate optimizer class\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 5 : train the model\n",
    "'''\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    \n",
    "    # 1) convert numpy to torch variables\n",
    "    \n",
    "    ########################\n",
    "    #  USE GPU FOR TENSOR  #\n",
    "    ########################\n",
    "    inputs = torch.from_numpy(x_train).to(device)\n",
    "    labels = torch.from_numpy(y_train).to(device)\n",
    "    \n",
    "    # 2) clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 3) foward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 4) calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 5) getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # 6) updatino parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch {0}, loss {1}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've learnt to...\n",
    "\n",
    "- Simple **linear regression basics**\n",
    "    - $y = Ax + B$\n",
    "    - $y = 2x + 1$\n",
    "- **Example** of simple linear regression\n",
    "- **Aim** of linear regression\n",
    "    - Minimizing distance between the points and the line\n",
    "        - Calculate \"distance\" through `MSE`\n",
    "        - Calculate `gradients`\n",
    "        - Update parameters with `parameters = parameters - learning_rate * gradients`\n",
    "        - Slowly update parameters $A$ and $B$ model the linear relationship between $y$ and $x$ of the form $y = 2x + 1$\n",
    "- Built a linear regression **model** in **CPU and GPU**\n",
    "    - Step 1: Create Model Class\n",
    "    - Step 2: Instantiate Model Class\n",
    "    - Step 3: Instantiate Loss Class\n",
    "    - Step 4: Instantiate Optimizer Class\n",
    "    - Step 5: Train Model\n",
    "- Important things to be on **GPU**\n",
    "    - `model`\n",
    "    - `tensors with gradients`\n",
    "- How to bring to **GPU**?\n",
    "    - `model_name.to(device)`  \n",
    "    - `variable_name.to(device)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with PyTorch\n",
    "## 1. About Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Logistic Regression Basics\n",
    "\n",
    "#### Classification algorithm\n",
    "- Example: Spam vs No Spam\n",
    "    - Input: Bunch of words\n",
    "    - Output: Probability spam or not\n",
    "\n",
    "#### Basic Comparison\n",
    "- **Linear regression**\n",
    "    - Output: numeric value given inputs\n",
    "- **Logistic regression**:\n",
    "    - Output: probability [0, 1] given input belonging to a class\n",
    "    \n",
    "    \n",
    "#### Input/Output Comparison\n",
    "- **Linear regression: Multiplication**\n",
    "    - Input: [1]\n",
    "        - Output: 2\n",
    "    - Input: [2]\n",
    "        - Output: 4\n",
    "    - Trying to model the relationship `y = 2x`\n",
    "- **Logistic regression: Spam**\n",
    "    - Input: \"Sign up to get 1 million dollars by tonight\"\n",
    "        - Output: p = 0.8\n",
    "    - Input: \"This is a receipt for your recent purchase with Amazon\"\n",
    "        - Output: p = 0.3\n",
    "    - **p: probability it is spam**\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Problems of Linear Regression\n",
    "- Example\n",
    "    - Fever\n",
    "    - **Input**: temperature\n",
    "    - **Output**: fever or no fever\n",
    "- Remember\n",
    "    - **Linear regression**: minimize error between points and line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1)**\n",
    "\n",
    "If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "x = [1, 5, 10, 10, 25, 50, 70, 75, 100,]\n",
    "y = [0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "colors = np.random.rand(len(x))\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "plt.ylabel(\"Fever\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression Problem 2: Fever points are not predicted with the presence of outliers**\n",
    "    \n",
    "Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1, 5, 10, 10, 25, 50, 70, 75, 300]\n",
    "y = [0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "colors = np.random.rand(len(x))\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "plt.ylabel(\"Fever\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Logistic Regression In-Depth\n",
    "\n",
    "#### Predicting Probability\n",
    "- Linear regression doesn't work\n",
    "- Instead of predicting direct values: **predict probability**\n",
    "- cross-entropy loss\n",
    "\n",
    "#### Logistic Function g()\n",
    "- **\"Two-class logistic regression\"**\n",
    "- $\\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b}$\n",
    "    - Where $\\boldsymbol{y}$ is a vector comprising the 2-class prediction $y_0$ and $y_1$\n",
    "    - Where the labels are $y_0 = 0$  and $y_1 = 1$\n",
    "    - Also, it's bolded because it's a vector, not a matrix.\n",
    "- $g(y_1) = \\frac {1} {1 + e^{-y_1}}$\n",
    "    - $g(y_1)$ = Estimated probability that $y = 1$\n",
    "- $g(y_0) = 1 - g(y_1)$\n",
    "    - $g(y_0)$ = Estimated probability that $y = 0$\n",
    "- For our illustration above, we have 4 classes, so we have to use softmax function explained below\n",
    "\n",
    "#### Softmax Function g()\n",
    "- **\"Multi-class logistic regression\"**\n",
    "    - Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem\n",
    "    - Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple.\n",
    "- $\\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b}$\n",
    "    - Where $\\boldsymbol{y}$ is a vector comprising the 4-class prediction $y_0, y_1, y_2, y_3$\n",
    "    - Where the 4 labels (K = 4) are $y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3$\n",
    "- $g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}}$ where K = 4 because we have 4 classes\n",
    "    - To put numbers to this equation in relation to the illustration above where we've $y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8$\n",
    "        - $g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017$\n",
    "        - $g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015$\n",
    "        - $g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412$\n",
    "        - $g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556$\n",
    "        - $g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0$\n",
    "        - All softmax outputs have to sum to one as they represent a probability distribution over K classes. \n",
    "- Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so).\n",
    "    - $y_0$ and $y_1$ are approximately similar in values and they return similar probabilities.\n",
    "    - Similarly, $y_2$ and $y_3$ are approximately similar in values and they return similar probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Function D() for 2 Class\n",
    "- Take note that here, $S$ is our softmax outputs and $L$ are our labels\n",
    "- $D(S, L) = -(L log S + (1-L)log(1-S))$\n",
    "    - If L = 0 (label)\n",
    "        - $D(S, 0) = - log(1-S)$\n",
    "            - $- log(1-S)$: less positive if $S \\longrightarrow 0$\n",
    "            - $- log(1-S)$: more positive if $S \\longrightarrow 1$ (BIGGER LOSS)\n",
    "    - If L = 1 (label)\n",
    "        - $D(S, 1) = - log S$\n",
    "            - $-log(S)$: less positive if $S \\longrightarrow 1$\n",
    "            - $-log(S)$: more positive if $S \\longrightarrow 0$ (BIGGER LOSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical example of bigger or small loss**\n",
    "\n",
    "You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000050000287824e-05\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(-math.log(1 - 0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.51292546497478\n"
     ]
    }
   ],
   "source": [
    "print(-math.log(1 - 0.99999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a small error of 1e-5 if your label is 1 and S is near 1 (very correct prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000050000287824e-05\n"
     ]
    }
   ],
   "source": [
    "print(-math.log(0.99999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a big error of 11.51 if your label is 1 and S is near 0 (very wrong prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.512925464970229\n"
     ]
    }
   ],
   "source": [
    "print(-math.log(0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Function D() for More Than 2 Class\n",
    "- For the case where we have more than 2 class, we need a more generalized function\n",
    "- $D(S, L) = - \\sum^K_1 L_i log(S_i)$\n",
    "    - $K$: number of classes\n",
    "    - $L_i$: label of i-th class, 1 if that's the class else 0\n",
    "    - $S_i$: output of softmax for i-th class\n",
    "\n",
    "#### Cross Entropy Loss over N samples\n",
    "- Goal: Minimizing Cross Entropy Loss, L\n",
    "- $Loss = \\frac {1}{N} \\sum_j^N D_j$\n",
    "    - $D_j$: j-th sample of cross entropy function $D(S, L)$\n",
    "    - $N$: number of samples\n",
    "    - $Loss$: average cross entropy loss over N samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Logistic Regression Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Loading MNIST Train Dataset\n",
    "#### Images from 1 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect length of training dataset**\n",
    "\n",
    "You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%| | 9658368/9912422 [00:12<00:00, 3103480.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/MNIST/MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/28881 [00:00<?, ?it/s]\u001b[A\n",
      "32768it [00:00, 47824.70it/s]                                                                                          \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                      | 0/1648877 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|                                                                      | 16384/1648877 [00:00<00:19, 85887.11it/s]\u001b[A\n",
      "  2%|                                                                     | 32768/1648877 [00:00<00:18, 86365.85it/s]\u001b[A\n",
      "  3%|                                                                     | 49152/1648877 [00:01<00:18, 88481.55it/s]\u001b[A\n",
      "  4%|                                                                    | 65536/1648877 [00:01<00:17, 90038.78it/s]\u001b[A\n",
      "  5%|                                                                   | 81920/1648877 [00:01<00:17, 91138.57it/s]\u001b[A\n",
      "  6%|                                                                  | 98304/1648877 [00:01<00:16, 92421.43it/s]\u001b[A\n",
      "  7%|                                                                 | 114688/1648877 [00:01<00:16, 93003.08it/s]\u001b[A\n",
      "  8%|                                                                | 131072/1648877 [00:01<00:16, 94055.37it/s]\u001b[A\n",
      "  9%|                                                               | 147456/1648877 [00:02<00:15, 94502.61it/s]\u001b[A\n",
      " 10%|                                                             | 172032/1648877 [00:02<00:14, 104446.66it/s]\u001b[A\n",
      " 12%|                                                            | 196608/1648877 [00:02<00:12, 113475.26it/s]\u001b[A\n",
      " 13%|                                                            | 212992/1648877 [00:02<00:13, 107477.77it/s]\u001b[A\n",
      " 14%|                                                           | 237568/1648877 [00:02<00:12, 115653.99it/s]\u001b[A\n",
      " 16%|                                                          | 262144/1648877 [00:02<00:11, 122486.33it/s]\u001b[A\n",
      " 17%|                                                         | 286720/1648877 [00:03<00:10, 128863.08it/s]\u001b[A\n",
      " 19%|                                                       | 319488/1648877 [00:03<00:09, 141169.77it/s]\u001b[A\n",
      " 21%|                                                      | 344064/1648877 [00:03<00:09, 141351.61it/s]\u001b[A\n",
      " 23%|                                                     | 376832/1648877 [00:03<00:08, 152475.60it/s]\u001b[A\n",
      " 25%|                                                   | 409600/1648877 [00:03<00:07, 162049.24it/s]\u001b[A\n",
      " 27%|                                                  | 442368/1648877 [00:04<00:07, 169535.70it/s]\u001b[A\n",
      " 29%|                                                 | 475136/1648877 [00:04<00:06, 176328.76it/s]\u001b[A\n",
      " 31%|                                               | 516096/1648877 [00:04<00:05, 190537.12it/s]\u001b[A\n",
      " 34%|                                             | 557056/1648877 [00:04<00:05, 205302.04it/s]\u001b[A\n",
      " 37%|                                           | 606208/1648877 [00:04<00:04, 223158.76it/s]\u001b[A\n",
      " 40%|                                         | 655360/1648877 [00:04<00:04, 239357.37it/s]\u001b[A\n",
      " 44%|                                      | 720896/1648877 [00:05<00:03, 264859.87it/s]\u001b[A\n",
      " 47%|                                    | 778240/1648877 [00:05<00:03, 287019.74it/s]\u001b[A\n",
      " 52%|                                 | 851968/1648877 [00:05<00:02, 314043.46it/s]\u001b[A\n",
      " 56%|                              | 925696/1648877 [00:05<00:02, 340546.17it/s]\u001b[A\n",
      " 61%|                          | 1007616/1648877 [00:05<00:01, 377364.31it/s]\u001b[A\n",
      " 67%|                      | 1097728/1648877 [00:05<00:01, 412219.28it/s]\u001b[A\n",
      " 73%|                  | 1196032/1648877 [00:06<00:01, 440893.03it/s]\u001b[A\n",
      " 79%|              | 1302528/1648877 [00:06<00:00, 482270.90it/s]\u001b[A\n",
      " 86%|         | 1417216/1648877 [00:06<00:00, 535751.65it/s]\u001b[A\n",
      " 94%|    | 1548288/1648877 [00:06<00:00, 576129.89it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "8192it [00:00, 16382.51it/s]                                                                                           \u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:30, 3103480.72it/s]                                                                                      \n",
      "1654784it [00:26, 576129.89it/s]                                                                                       \u001b[A"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting a single image**\n",
    "\n",
    "So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting a single data point in the training dataset**\n",
    "\n",
    "When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting training dataset first element of tuple**\n",
    "\n",
    "This means to access the image, you need to access the first element in the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input matrix\n",
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting training dataset second element of tuple**\n",
    "\n",
    "The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label\n",
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifying shape of MNIST image**\n",
    "\n",
    "As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot image of MNIST image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_img = train_dataset[0][0].numpy().reshape(28, 28)\n",
    "show_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b5d157c780>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(show_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second element of tuple shows label**\n",
    "\n",
    "As you would expect, the label is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label\n",
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot second image of MNIST image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img = train_dataset[1][0].numpy().reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b5d1da9048>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(show_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second element of tuple shows label**\n",
    "\n",
    "We should see 0 here as the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label\n",
    "train_dataset[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b: Loading MNIST Test Dataset\n",
    "- Show our algorithm works beyond the data we have trained on.\n",
    "- Out-of-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load test dataset**\n",
    "\n",
    "Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dsets.MNIST(root='./data/MNIST/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test dataset elements**\n",
    "\n",
    "Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test dataset first element in tuple**\n",
    "\n",
    "This contains the image matrix, similar to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image matrix\n",
    "test_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot image sample from test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b5c7e367f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img = test_dataset[0][0].numpy().reshape(28, 28)\n",
    "plt.imshow(show_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test dataset second element in tuple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label\n",
    "test_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Dataset Iterable\n",
    "- Aim: make the dataset iterable\n",
    "- **totaldata**: 60000\n",
    "- **minibatch**: 100\n",
    "    - Number of examples in 1 iteration\n",
    "- **iterations**: 3000\n",
    "    - 1 iteration: one mini-batch forward & backward pass\n",
    "- **epochs**\n",
    "    - 1 epoch: running through the whole dataset once\n",
    "    - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap training dataset**\n",
    "\n",
    "Remember training dataset has 60k images and testing dataset has 10k images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining epochs**\n",
    "\n",
    "When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch.\n",
    "\n",
    "However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arbitrarily set 3000 iterations here which means the model would update 3000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Iterable Object: Training Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterable object\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Iterability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "isinstance(train_loader, collections.Iterable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Iterable Object: Testing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterable object\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check iterability of testing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(test_loader, collections.Iterable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterate through dataset**\n",
    "\n",
    "This is just a simplified example of what we're doing above where we're creating an iterable object `lst` to loop through so we can access all the images `img_1` and `img_2`.\n",
    "    \n",
    "Above, the equivalent of `lst` is `train_loader` and `test_loader`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = np.ones((28, 28))\n",
    "img_2 = np.ones((28, 28))\n",
    "lst = [img_1, img_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [img_1, img_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as linear regression\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate Model Class\n",
    "- Input dimension: \n",
    "    - Size of image\n",
    "    - $28 \\times 28 = 784$\n",
    "- Output dimension: 10\n",
    "    - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check size of dataset**\n",
    "\n",
    "This should be 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of images\n",
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate model class based on input and out dimensions**\n",
    "\n",
    "As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10.\n",
    "\n",
    "And we're feeding the model with 28x28 images, hence our input dimension is 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate Loss Class\n",
    "- **Logistic Regression**: Cross Entropy Loss\n",
    "    - _Linear Regression: MSE_\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Cross Entry Loss Class**\n",
    "\n",
    "Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens in `nn.CrossEntropyLoss()`?**\n",
    "\n",
    "It does 2 things at the same time.\n",
    "\n",
    "\n",
    "1. Computes softmax (logistic/softmax function) \n",
    "2. Computes cross entropy\n",
    "\n",
    "<img src=\"./images/lab2/cross_entropy_final_4.png\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Instantiate Optimizer Class\n",
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n",
    "        - $\\theta$: parameters (our variables)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    - **At every iteration, we update our model's parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create optimizer**\n",
    "\n",
    "Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters In-Depth**\n",
    "\n",
    "\n",
    "You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the $y = AX + b$ equation where X is our input of size 784.\n",
    "    \n",
    "We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001B5D1729228>\n"
     ]
    }
   ],
   "source": [
    "# Type of parameter object\n",
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Length of parameters\n",
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0081, -0.0302,  0.0109,  ..., -0.0080, -0.0031,  0.0010],\n",
       "         [ 0.0117, -0.0268,  0.0348,  ...,  0.0001, -0.0253, -0.0291],\n",
       "         [-0.0126, -0.0150,  0.0332,  ..., -0.0253, -0.0311, -0.0002],\n",
       "         ...,\n",
       "         [-0.0056, -0.0271,  0.0261,  ..., -0.0053, -0.0354, -0.0263],\n",
       "         [ 0.0092,  0.0335, -0.0316,  ..., -0.0327, -0.0076,  0.0129],\n",
       "         [-0.0091,  0.0006, -0.0238,  ..., -0.0337,  0.0049,  0.0021]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0110,  0.0082,  0.0130, -0.0064,  0.0203,  0.0019,  0.0278,  0.0326,\n",
       "         -0.0249, -0.0114], requires_grad=True)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Matrix Product Review**\n",
    "\n",
    "- Example 1: **matrix product**\n",
    "    - $A: (100, 10)$\n",
    "    - $B: (10, 1)$\n",
    "    - $A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1)$\n",
    "- Example 2: **matrix product**\n",
    "    - $A: (50, 5)$\n",
    "    - $B: (5, 2)$\n",
    "    - $A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2)$\n",
    "- Example 3: **element-wise addition**\n",
    "    - $A: (10, 1)$\n",
    "    - $B: (10, 1)$\n",
    "    - $A + B = (10, 1)$\n",
    "        \n",
    "<img src=\"./images/lab2/lr_params2.png\" alt=\"no_image\" style=\"width: 900px;\"/>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Model\n",
    "\n",
    "**7 step process for training models**\n",
    "\n",
    " - Process \n",
    "    1. Convert inputs/labels to tensors with gradients\n",
    "    2. Clear gradient buffets\n",
    "    3. Get output given inputs\n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. parameters\n",
    "    6. Update parameters using gradients\n",
    "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 500, loss: 1.5914448499679565, accuracy: 76.44\n",
      "iteration: 1000, loss: 1.3247126340866089, accuracy: 79.75\n",
      "iteration: 1500, loss: 1.1901869773864746, accuracy: 81.57\n",
      "iteration: 2000, loss: 1.1496357917785645, accuracy: 82.5\n",
      "iteration: 2500, loss: 1.0168027877807617, accuracy: 83.2\n",
      "iteration: 3000, loss: 1.0037602186203003, accuracy: 83.77\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # load images as variable\n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        # clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # calculate loss : softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # load images to a torch variable\n",
    "                images = images.view(-1, 28*28)\n",
    "                \n",
    "                # forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct.item() / total\n",
    "            \n",
    "            # print loss\n",
    "            print('iteration: {}, loss: {}, accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break Down Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing outputs of our model**\n",
    "\n",
    "As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model.\n",
    "\n",
    "This would print out the output of the model's predictions on your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs\n",
      "tensor([[-4.1912e-01, -1.1316e+00, -3.3780e-01,  5.0849e-02,  1.6380e-01,\n",
      "         -4.5955e-01, -1.1082e+00,  3.2220e+00, -9.9257e-02,  8.3546e-01],\n",
      "        [ 6.2732e-01,  5.0818e-02,  1.9713e+00,  1.1428e+00, -1.7106e+00,\n",
      "          6.1820e-01,  1.4372e+00, -1.6908e+00,  4.6356e-01, -1.4948e+00],\n",
      "        [-9.5425e-01,  2.4788e+00,  1.7959e-01,  1.3081e-01, -6.0669e-01,\n",
      "         -2.7401e-01, -1.0602e-01, -4.2117e-01,  1.6995e-01, -4.4849e-01],\n",
      "        [ 3.2173e+00, -2.5713e+00,  5.0179e-02, -9.7821e-02, -8.8109e-01,\n",
      "          4.6438e-01,  1.1085e+00,  2.5932e-01, -4.3773e-01, -4.8476e-01],\n",
      "        [-2.3028e-01, -2.0807e+00,  4.7810e-01, -6.9463e-01,  1.8244e+00,\n",
      "         -3.3407e-01,  3.2245e-01,  3.8161e-01, -9.6695e-03,  9.6337e-01],\n",
      "        [-1.3049e+00,  3.0202e+00,  1.7652e-01,  4.1544e-01, -6.7183e-01,\n",
      "         -3.8085e-01, -6.9530e-01, -8.5968e-02,  4.6802e-01, -2.5801e-01],\n",
      "        [-1.2533e+00, -1.2030e+00, -9.5699e-01,  9.7894e-02,  1.3802e+00,\n",
      "          5.0112e-01, -7.2388e-01,  6.0730e-01,  6.7461e-01,  9.4082e-01],\n",
      "        [-1.4455e+00, -4.5451e-01, -6.0625e-01, -1.7707e-01,  8.5962e-01,\n",
      "          4.1746e-01,  3.1149e-01,  1.4762e-01, -2.5914e-01,  1.5603e+00],\n",
      "        [ 4.7325e-01, -5.6498e-01,  7.8519e-01, -1.4749e+00,  8.5451e-01,\n",
      "          9.1588e-02,  9.7697e-01, -1.0214e+00,  1.6777e-01,  1.7729e-01],\n",
      "        [-6.0699e-01, -1.0353e+00, -1.1757e+00, -1.0584e+00,  1.2649e+00,\n",
      "         -9.0552e-03, -4.2450e-01,  1.9525e+00,  1.6006e-01,  2.0453e+00],\n",
      "        [ 3.3368e+00, -2.1774e+00,  4.2498e-01,  1.1774e+00, -1.1876e+00,\n",
      "          1.2601e+00, -5.8377e-01, -1.6138e+00,  7.1909e-01, -1.9385e+00],\n",
      "        [ 7.6137e-01, -5.2252e-01,  6.1700e-01,  1.7191e-01,  3.5332e-01,\n",
      "         -2.9286e-01,  7.7696e-01, -1.1822e+00,  1.3439e-01, -5.1545e-01],\n",
      "        [-8.2270e-01, -1.6279e+00, -7.7600e-01, -1.0651e+00,  1.7023e+00,\n",
      "          1.0223e-01, -3.3829e-01,  1.1943e+00,  2.3789e-01,  2.4292e+00],\n",
      "        [ 3.3211e+00, -2.6864e+00, -2.1372e-01, -9.0289e-02, -4.9209e-01,\n",
      "          7.8214e-01, -3.5149e-01, -6.9174e-01,  7.0809e-01,  1.4179e-01],\n",
      "        [-1.4724e+00,  3.1911e+00, -5.7965e-02,  5.2246e-01, -1.0297e+00,\n",
      "         -1.5703e-01,  7.9587e-02, -1.9835e-01,  5.4452e-01, -2.6694e-01],\n",
      "        [ 5.2042e-01, -8.7574e-01,  1.7546e-01,  1.3895e+00, -4.9154e-01,\n",
      "          1.0818e+00, -4.3420e-01, -4.1187e-01,  5.0001e-01, -7.6288e-01],\n",
      "        [-4.6051e-01, -2.1806e+00,  8.6544e-02, -5.4227e-01,  1.4769e+00,\n",
      "         -6.2969e-01, -2.5993e-01,  9.1181e-01,  1.8792e-01,  1.9047e+00],\n",
      "        [ 2.3124e-01, -1.5122e+00, -5.9722e-01,  7.6301e-01, -5.1682e-02,\n",
      "         -3.7827e-01, -8.1703e-01,  3.0985e+00, -3.1228e-01,  4.0048e-01],\n",
      "        [-8.4482e-01, -4.7905e-01,  3.9815e-01,  1.2329e+00, -1.0372e-01,\n",
      "          6.8402e-01,  1.0868e+00, -2.2882e-01,  3.2965e-01, -4.0243e-01],\n",
      "        [-9.4391e-01, -1.7276e+00, -4.0278e-01, -1.1966e-01,  2.2852e+00,\n",
      "          2.1449e-01, -1.7745e-01,  1.3102e-01,  2.1835e-02,  1.5759e+00],\n",
      "        [-1.0373e+00, -2.9585e-01, -1.5319e+00,  1.5993e-01,  6.0266e-01,\n",
      "          4.0021e-01, -1.3646e+00,  1.8198e+00,  3.6132e-01,  1.7384e+00],\n",
      "        [-5.5941e-01, -1.7121e+00,  2.0425e-01,  2.6633e-01,  4.5068e-01,\n",
      "          7.7493e-01,  2.5108e+00, -1.4030e+00,  1.6171e-01,  7.4696e-02],\n",
      "        [-4.2802e-01,  1.8119e-01,  3.4219e-01, -6.2464e-01,  1.0846e+00,\n",
      "         -1.2669e+00,  1.2606e+00, -3.8034e-02, -9.0451e-02,  4.4966e-02],\n",
      "        [ 6.2167e-02, -9.9991e-01, -7.6377e-01,  3.2699e-01,  4.0359e-02,\n",
      "          1.5582e+00,  3.5914e-01, -8.8420e-01,  8.6610e-01,  3.9126e-02],\n",
      "        [-6.5962e-01, -1.2025e+00,  1.6330e-01, -1.9066e-01,  1.5528e+00,\n",
      "         -1.7345e-01, -1.6236e-01,  2.9581e-01, -3.4507e-01,  1.2556e+00],\n",
      "        [ 4.7904e+00, -3.4040e+00,  4.2370e-01, -1.0601e+00, -3.7085e-01,\n",
      "          1.3357e+00,  1.0549e+00, -8.2278e-01, -4.1026e-04, -1.4678e+00],\n",
      "        [-6.6799e-02, -1.3020e+00, -2.3578e-01,  2.8357e-01,  4.3582e-01,\n",
      "          1.2899e-01, -4.4850e-01,  2.0931e+00, -3.7223e-01,  1.2334e+00],\n",
      "        [-3.6855e-01, -2.5590e+00, -2.0625e-01, -7.2516e-01,  2.6218e+00,\n",
      "          3.5260e-01,  2.9415e-01, -8.1790e-02,  1.2593e-01,  1.8299e+00],\n",
      "        [ 3.0195e+00, -2.6403e+00,  2.8003e-01,  1.2124e+00, -1.2222e+00,\n",
      "          8.4078e-01, -3.4647e-01, -6.5245e-01,  9.0016e-01, -7.3976e-01],\n",
      "        [-1.4519e+00,  1.7294e+00, -2.9714e-01,  2.0062e-01, -5.3016e-01,\n",
      "          3.4469e-01,  2.1558e-01, -1.1763e-01,  6.0452e-01, -1.8329e-01],\n",
      "        [-1.0631e+00, -1.4597e-03, -9.3382e-01,  2.4709e+00, -1.2374e+00,\n",
      "          8.9352e-01, -2.8077e-01,  5.5966e-01,  2.0299e-01,  3.0195e-02],\n",
      "        [-1.3204e+00,  1.3536e+00, -3.3830e-01,  3.2217e-01, -3.4384e-01,\n",
      "          2.5983e-01, -1.2871e-02,  2.1188e-01,  3.3446e-01,  2.5807e-01],\n",
      "        [-1.0915e+00, -8.6311e-01, -6.7506e-01,  2.6588e+00, -3.6921e-01,\n",
      "          1.4474e+00, -1.6185e-01, -9.5622e-01,  5.6326e-01,  4.5016e-02],\n",
      "        [ 1.6910e+00, -2.2178e+00,  9.0904e-01, -1.6205e+00,  9.8873e-01,\n",
      "          2.1559e-01,  1.6210e+00, -1.0175e+00, -3.4075e-01, -2.5803e-01],\n",
      "        [-9.6022e-01, -2.7629e-01,  8.5401e-01, -2.7186e-01, -6.5955e-02,\n",
      "         -6.0811e-01, -1.5388e+00,  1.9722e+00,  6.1696e-01,  7.0457e-01],\n",
      "        [ 4.5295e-01, -1.0565e+00,  2.7443e+00,  2.5321e-02, -1.0920e+00,\n",
      "          3.7110e-03,  3.4909e-01, -2.8396e-01,  4.5228e-01, -1.8196e+00],\n",
      "        [-5.6216e-01, -1.4793e+00, -1.1836e-02,  2.5185e-02, -3.7413e-02,\n",
      "         -4.1448e-01, -7.5911e-01,  2.4851e+00, -5.4995e-02,  1.0609e+00],\n",
      "        [-1.6288e+00,  2.4302e+00, -4.3604e-01,  7.5702e-02, -6.2815e-01,\n",
      "          1.9120e-01,  6.2220e-02,  1.5604e-03,  6.2171e-01,  1.9619e-02],\n",
      "        [ 3.7386e-01,  5.7086e-01,  1.2225e+00,  1.1792e+00, -2.0127e+00,\n",
      "          4.4378e-01,  5.9156e-01, -1.1577e+00,  8.1110e-01, -1.5477e+00],\n",
      "        [-1.5716e+00,  3.3043e+00, -1.1821e-01,  4.9650e-01, -1.3200e+00,\n",
      "         -4.5151e-02, -6.1483e-02, -6.4991e-01,  8.7302e-01, -2.7298e-01],\n",
      "        [-7.5706e-01,  1.8713e+00, -9.6020e-02,  8.2175e-02, -4.8900e-01,\n",
      "         -1.4591e-01, -4.4427e-02, -1.9235e-01,  1.6672e-01, -1.2079e-01],\n",
      "        [-8.4616e-01, -8.5063e-01, -2.8629e-01, -2.3284e-01,  2.8741e-01,\n",
      "         -3.7657e-01, -5.1174e-01,  2.3083e+00, -2.7671e-01,  1.4264e+00],\n",
      "        [-2.1613e+00, -3.4893e-01, -2.0523e-01, -3.1229e-01,  2.4073e+00,\n",
      "         -5.6236e-01, -8.8912e-01,  6.5524e-01,  5.2819e-01,  1.9805e+00],\n",
      "        [-3.7676e-01,  8.7755e-01,  1.2935e+00,  8.5823e-02, -7.8217e-02,\n",
      "         -3.7754e-03,  4.4668e-01, -1.1829e+00,  5.1375e-01, -3.9273e-01],\n",
      "        [-1.2719e+00,  2.5289e-01,  6.1618e-02,  1.3166e+00, -5.9522e-01,\n",
      "          5.0579e-01,  6.4527e-01, -4.0225e-01,  1.5707e-01, -2.9673e-01],\n",
      "        [ 1.2345e-01, -1.4061e+00, -5.5321e-01,  1.7273e+00, -6.7476e-01,\n",
      "          1.4482e+00,  8.7405e-02, -1.3274e+00,  1.1319e+00, -4.9011e-01],\n",
      "        [-2.0010e+00,  4.5560e-01,  2.4998e-03,  6.9747e-01, -7.7570e-02,\n",
      "          5.5163e-01,  3.0411e-01, -1.2403e-01,  4.5624e-01,  3.5361e-01],\n",
      "        [-6.9439e-01, -3.0562e-01,  1.6930e+00, -3.0058e-01,  4.5233e-01,\n",
      "         -6.9403e-01,  4.2407e-01, -2.1024e-01, -1.3702e-01,  5.9118e-02],\n",
      "        [-1.1275e+00, -2.9608e+00, -1.4641e+00,  6.8846e-02,  3.0236e+00,\n",
      "          8.3263e-01, -5.7946e-01,  3.2541e-01,  8.4300e-01,  2.5088e+00],\n",
      "        [-1.0354e-01, -2.1613e+00,  6.5008e-01, -5.4618e-01,  2.5685e+00,\n",
      "         -8.9262e-01,  2.8397e-01,  4.2691e-01, -2.1991e-01,  1.2836e+00],\n",
      "        [ 1.3179e-01, -7.8202e-01,  2.6453e-01,  2.5189e-01, -3.0590e-01,\n",
      "          6.2477e-01,  2.4491e+00, -9.9831e-01, -1.4772e-01, -5.3560e-01],\n",
      "        [-7.6129e-02, -9.8545e-01, -1.1744e-01,  1.9654e+00, -3.5157e-01,\n",
      "          7.3227e-01,  2.1464e-02, -1.4486e-01, -1.6106e-01, -4.2379e-03],\n",
      "        [ 7.9608e-01, -1.3417e+00, -1.2955e+00,  1.2432e-01,  8.7443e-01,\n",
      "          1.3913e+00, -1.5050e-01, -1.5523e-02, -1.1752e-01,  5.7715e-01],\n",
      "        [ 4.3793e-01, -7.2015e-01, -3.7907e-01,  1.1130e+00, -8.2845e-02,\n",
      "          7.6299e-01, -7.4092e-02, -6.5644e-01,  4.9862e-01, -3.8459e-01],\n",
      "        [ 3.5058e-02, -8.0895e-01,  1.3838e+00, -1.9466e-02,  9.1705e-01,\n",
      "         -8.8315e-01,  8.1102e-01, -5.9129e-01,  1.2895e-01, -3.7538e-01],\n",
      "        [ 1.6878e+00, -2.4385e+00, -3.7394e-01,  6.3863e-01, -7.4088e-01,\n",
      "          1.0496e+00,  2.8428e-01, -8.3585e-01,  1.6021e+00, -3.4641e-01],\n",
      "        [ 6.5436e-02, -2.8450e+00, -2.1493e-02, -5.1064e-01,  2.9940e+00,\n",
      "          1.7491e-01,  5.3527e-01, -1.6741e-01,  1.2059e-02,  1.2198e+00],\n",
      "        [-9.7253e-01,  2.6178e+00,  5.6102e-02,  3.7613e-01, -8.0450e-01,\n",
      "         -3.0679e-01, -4.6103e-01, -2.5825e-01,  3.1205e-01, -2.2237e-01],\n",
      "        [-1.0495e-01, -2.1089e+00, -7.0706e-01, -7.7735e-01,  1.9767e+00,\n",
      "         -2.4356e-02, -4.7600e-02,  1.2948e+00, -1.9322e-01,  2.3952e+00],\n",
      "        [ 2.2526e-01,  9.9757e-02, -3.9357e-01, -4.7296e-01,  1.8260e-01,\n",
      "          4.0007e-01, -2.6676e-01,  3.3490e-01,  1.7865e-01, -1.5882e-01],\n",
      "        [-1.4491e-01, -1.9817e+00, -4.8034e-01,  9.0569e-01,  5.4758e-01,\n",
      "          1.8450e-01, -5.2265e-02,  2.4180e+00, -5.6242e-01,  5.1134e-01],\n",
      "        [ 9.1469e-02, -9.5583e-01,  1.1879e+00, -1.6101e+00, -8.4008e-02,\n",
      "          9.0452e-02,  5.9678e-01, -9.1174e-01,  1.5013e+00,  4.2024e-01],\n",
      "        [-8.0225e-01, -8.3416e-01, -2.9717e-02, -3.4594e-01,  8.5678e-01,\n",
      "          2.5405e-01,  8.9586e-02, -4.4238e-02,  4.7185e-01,  1.2080e+00],\n",
      "        [-9.7419e-01, -1.2624e-01,  1.2263e+00,  6.9177e-01, -1.5486e-01,\n",
      "         -1.8423e-01, -3.5320e-01, -7.7899e-01,  6.3721e-01,  1.7518e-01],\n",
      "        [-8.5579e-01, -7.8018e-01,  4.7096e-01, -5.1016e-01,  1.0002e+00,\n",
      "         -5.3595e-01, -3.4266e-01,  1.8964e+00,  4.0244e-02,  9.7955e-01],\n",
      "        [-1.4202e+00, -7.0722e-01, -7.1879e-01,  5.5143e-01,  7.1155e-01,\n",
      "          6.9051e-01, -2.5288e-02, -2.9470e-01,  5.1833e-01,  8.8556e-01],\n",
      "        [ 2.5748e-01, -1.2507e-01,  6.7579e-01, -1.9532e-01,  4.3748e-01,\n",
      "         -6.4743e-01,  8.4237e-01,  2.3026e-01, -3.1508e-01, -4.3352e-01],\n",
      "        [-4.9358e-01, -1.5060e+00,  6.1699e-01, -1.1035e+00,  2.4550e+00,\n",
      "         -6.2610e-01, -2.2161e-01,  2.7537e-01,  2.8175e-01,  1.0041e+00],\n",
      "        [-1.4875e+00, -4.3237e-01, -3.5084e-01,  3.0687e+00, -5.0584e-01,\n",
      "          9.2293e-01, -9.8310e-01, -9.4573e-01,  1.1361e+00,  1.7931e-01],\n",
      "        [ 2.7851e+00, -1.8097e+00,  5.4764e-01, -7.0944e-01, -1.3629e+00,\n",
      "          8.1827e-01,  5.2995e-01,  1.3905e-01, -2.8294e-01, -4.6628e-01],\n",
      "        [ 4.5395e-01, -1.4516e+00, -7.9688e-01,  3.3955e-01,  1.7537e-01,\n",
      "         -4.8508e-01, -7.0649e-01,  3.1589e+00, -1.8786e-01,  2.7803e-01],\n",
      "        [ 4.6083e+00, -3.4235e+00,  8.2347e-01,  4.0148e-01, -1.2484e+00,\n",
      "          1.2900e+00,  1.9452e-02, -1.6528e+00,  5.1607e-01, -1.8512e+00],\n",
      "        [ 1.6007e+00, -1.2984e+00,  1.9017e+00,  1.4904e+00, -1.0116e+00,\n",
      "          2.7393e-01,  4.1729e-01, -8.2427e-01,  4.0971e-01, -1.8764e+00],\n",
      "        [-1.4493e+00,  1.0431e+00,  5.5839e-01, -9.4288e-02, -8.7936e-01,\n",
      "         -2.9928e-01, -9.3427e-01,  1.0646e+00,  1.2822e+00,  7.6258e-01],\n",
      "        [-1.5842e+00,  2.6825e+00, -4.0441e-01,  2.2652e-02, -8.9432e-01,\n",
      "          1.3008e-01,  1.3930e-01, -1.1685e-01,  8.2070e-01, -1.1933e-01],\n",
      "        [-1.7622e+00,  7.3900e-01, -4.4466e-01, -4.2134e-01,  9.7232e-01,\n",
      "         -3.0708e-01, -6.7011e-01,  1.9577e+00,  2.7517e-01,  9.0359e-01],\n",
      "        [-3.6511e-01,  6.7307e-02, -3.5746e-01,  2.6027e+00, -1.2895e+00,\n",
      "          1.0839e+00, -4.5193e-01, -1.0644e+00,  6.0937e-01, -7.9455e-01],\n",
      "        [-5.4875e-01, -2.8995e-01,  8.2005e-01, -6.0697e-01, -5.5061e-02,\n",
      "         -7.4545e-01,  1.4824e-01,  2.0123e+00, -2.6959e-01,  6.7178e-01],\n",
      "        [-1.8328e+00,  1.0870e+00, -7.8371e-01,  2.6527e-01, -1.3205e-01,\n",
      "          1.2209e-01, -5.7470e-01,  4.1548e-01,  1.1237e+00,  7.0836e-01],\n",
      "        [-3.5698e-01, -6.0508e-01, -3.5238e-01, -1.0218e+00,  1.6555e-01,\n",
      "         -3.5592e-01, -1.3701e+00,  3.9065e+00,  3.9650e-01,  1.1204e+00],\n",
      "        [-5.0983e-01, -1.9453e+00, -1.0462e+00, -2.5920e-01,  1.1990e+00,\n",
      "          6.8558e-01, -1.6164e-01,  1.6784e+00, -7.8997e-01,  2.1020e+00],\n",
      "        [ 1.5367e-01, -1.7261e+00,  6.6260e-01, -5.8606e-01,  3.3931e-01,\n",
      "          3.6214e-01,  2.6821e+00, -4.2454e-01, -1.8095e-01,  1.0432e-01],\n",
      "        [-5.7142e-01, -1.2186e+00,  4.4039e+00, -6.1011e-02, -2.8471e-01,\n",
      "         -1.3287e+00,  8.8264e-01, -9.8884e-01,  6.5963e-01, -1.2045e+00],\n",
      "        [-5.3560e-01, -1.7884e+00, -5.0931e-01,  8.1561e-03,  9.5256e-01,\n",
      "         -2.2075e-02, -7.8659e-01,  2.4689e+00, -3.7056e-01,  1.7113e+00],\n",
      "        [-5.1876e-01, -1.9370e-01, -4.1295e-01, -5.5168e-01,  5.7175e-01,\n",
      "          7.4202e-01, -4.4452e-01, -7.9225e-01,  9.9781e-01,  7.8333e-01],\n",
      "        [-9.0510e-01, -2.6228e+00, -9.9571e-01,  1.7336e-01,  3.1362e+00,\n",
      "          4.7385e-01, -1.3938e-01, -4.3341e-01,  4.1269e-01,  1.6844e+00],\n",
      "        [-1.9506e+00,  6.7284e-01, -2.3458e-01, -4.3837e-01,  1.0537e-01,\n",
      "         -1.0105e+00, -1.1321e+00,  3.2299e+00,  4.0666e-01,  1.1506e+00],\n",
      "        [-2.1408e-01, -1.5895e+00, -1.0596e+00,  2.0395e+00, -1.7724e-01,\n",
      "          1.4714e+00,  1.0129e+00, -6.2490e-01,  2.3076e-02, -1.4023e-01],\n",
      "        [-1.8088e-01, -2.1159e+00,  1.1151e+00, -1.2247e+00,  1.4110e+00,\n",
      "         -5.2799e-01,  2.8360e+00,  1.0243e-01, -6.4501e-01,  5.4443e-01],\n",
      "        [-1.5710e+00,  3.2571e+00,  4.8242e-01,  2.6691e-01, -7.2919e-01,\n",
      "         -3.2589e-01, -2.4873e-01, -3.9496e-01,  5.0746e-01, -4.3707e-01],\n",
      "        [ 1.2125e-01, -8.1265e-01, -2.4051e-01,  3.2313e+00, -1.4580e+00,\n",
      "          1.0410e+00, -1.5254e+00, -4.9155e-01,  9.9198e-01, -8.0305e-01],\n",
      "        [-9.0882e-01, -5.1786e-01,  4.2862e-01, -8.4637e-01,  5.3394e-01,\n",
      "         -1.4025e-01,  2.9691e+00, -1.0820e+00,  3.4564e-01,  1.1133e-01],\n",
      "        [-8.8621e-01,  1.6725e-01,  1.3282e-01, -5.2387e-01,  4.7761e-01,\n",
      "          3.1391e-02, -1.5354e-01,  1.3434e-01,  6.5787e-01,  6.5513e-01],\n",
      "        [-1.1496e+00, -5.4029e-01, -6.3112e-01,  2.7948e+00, -1.5379e+00,\n",
      "          1.2394e+00, -1.0944e+00,  6.3031e-01,  8.3630e-01,  1.4267e-02],\n",
      "        [-2.6532e+00,  1.9910e+00,  5.3068e-02,  1.6248e-01, -4.9459e-01,\n",
      "          2.5385e-01,  6.2841e-01, -5.0900e-01,  1.1298e+00,  1.5831e-01],\n",
      "        [-8.2772e-01, -5.9038e-01, -2.4802e-01, -1.4479e+00,  2.6656e+00,\n",
      "         -1.7681e-01,  5.1755e-01, -1.1725e-01,  1.2863e-01,  1.6894e+00],\n",
      "        [-1.5174e+00,  6.4386e-01, -4.9193e-01,  3.1739e-01, -9.5386e-02,\n",
      "          5.3998e-01,  1.1092e-02,  8.7619e-02,  4.4793e-01,  3.2142e-01],\n",
      "        [-2.0309e+00,  1.6202e+00, -6.6545e-01,  2.4791e-01, -1.8524e-01,\n",
      "          4.9144e-02,  2.0093e-01,  8.0732e-01,  3.0584e-01,  5.8670e-01],\n",
      "        [ 1.0073e+00, -1.2560e+00,  9.7645e-01, -8.8159e-01, -1.2789e-01,\n",
      "          3.6903e-01,  2.0528e+00, -1.1783e+00, -1.2817e-01, -6.7364e-01],\n",
      "        [-1.1287e+00, -1.8177e+00,  2.4763e-01, -5.9602e-01,  1.8297e+00,\n",
      "         -9.1682e-01, -1.3049e-01,  1.0548e+00,  1.1826e-01,  2.6632e+00]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0\n",
    "for images, labels in test_loader:\n",
    "    iter_test += 1\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    if iter_test == 1:\n",
    "        print('Outputs')\n",
    "        print(outputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing output size**\n",
    "\n",
    "This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0\n",
    "for images, labels in test_loader:\n",
    "    iter_test += 1\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    if iter_test == 1:\n",
    "        print('Outputs')\n",
    "        print(outputs.size())\n",
    "    _, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing one output**\n",
    "\n",
    "This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7.\n",
    "\n",
    "number 0: -0.4876\n",
    "\n",
    "number 1: -2.2708\n",
    "\n",
    "... \n",
    "\n",
    "number 7: 4.6617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS\n",
      "tensor([-0.4191, -1.1316, -0.3378,  0.0508,  0.1638, -0.4596, -1.1082,  3.2220,\n",
      "        -0.0993,  0.8355], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0\n",
    "for images, labels in test_loader:\n",
    "    iter_test += 1\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    if iter_test == 1:\n",
    "        print('OUTPUTS')\n",
    "        print(outputs[0, :])\n",
    "    _, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing prediction output**\n",
    "\n",
    "Because our output is of size 100 (our batch size), our prediction size would also of the size 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0\n",
    "for images, labels in test_loader:\n",
    "    iter_test += 1\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if iter_test == 1:\n",
    "        print('PREDICTION')\n",
    "        print(predicted.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print prediction value**\n",
    "\n",
    "We are printing our prediction which as verified above, should be digit 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0\n",
    "for images, labels in test_loader:\n",
    "    iter_test += 1\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if iter_test == 1:\n",
    "        print('PREDICTION')\n",
    "        print(predicted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print prediction, label and label size**\n",
    "\n",
    "We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "tensor(7)\n",
      "\n",
      "\n",
      "LABEL SIZE\n",
      "torch.Size([100])\n",
      "\n",
      "\n",
      "LABEL FOR IMAGE 0\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0\n",
    "for images, labels in test_loader:\n",
    "    iter_test += 1\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if iter_test == 1:\n",
    "        print('PREDICTION')\n",
    "        print(predicted[0])\n",
    "        print('\\n')\n",
    "        print('LABEL SIZE')\n",
    "        print(labels.size())\n",
    "        print('\\n')\n",
    "        print('LABEL FOR IMAGE 0')\n",
    "        print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print second prediction and ground truth**\n",
    "\n",
    "Again, the prediction is correct. Naturally, as our model is quite competent in this simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "tensor(2)\n",
      "LABEL SIZE\n",
      "torch.Size([100])\n",
      "LABEL FOR IMAGE 1\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0\n",
    "for images, labels in test_loader:\n",
    "    iter_test += 1\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    if iter_test == 1:\n",
    "        print('PREDICTION')\n",
    "        print(predicted[1])\n",
    "\n",
    "        print('LABEL SIZE')\n",
    "        print(labels.size())\n",
    "\n",
    "        print('LABEL FOR IMAGE 1')\n",
    "        print(labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print accuracy**\n",
    "\n",
    "Now we know what each object represents, we can understand how we arrived at our accuracy numbers.\n",
    "    \n",
    "One last thing to note is that `correct.item()` has this syntax is because `correct` is a PyTorch tensor and to get the value to compute with `total` which is an integer, we need to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.77\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "iter_test = 0\n",
    "for images, labels in test_loader:\n",
    "    iter_test += 1\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # total number of labels\n",
    "    total += labels.size(0)\n",
    "    \n",
    "    # total correct predictions\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "accuracy = 100 * (correct.item() / total)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Python's `.sum()` function**\n",
    "\n",
    "Python's `.sum()` function allows you to do a comparison between two matrices and sum the ones that return `True` or in our case, those predictions that match actual labels (correct predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones((10))\n",
    "print(a)\n",
    "b = np.ones((10))\n",
    "print(b)\n",
    "\n",
    "print(a == b)\n",
    "\n",
    "print((a == b).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving PyTorch model**\n",
    "\n",
    "This is how you save your model. Feel free to just change `save_model = True` to save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model is True:\n",
    "    torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Logistic Regression Model with PyTorch (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CPU version**\n",
    "\n",
    "The usual 7-step process, getting repetitive by now which we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 500, loss: 1.799738883972168, accuracy: 65.65\n",
      "iteration: 1000, loss: 1.5458807945251465, accuracy: 74.9\n",
      "iteration: 1500, loss: 1.3627903461456299, accuracy: 78.62\n",
      "iteration: 2000, loss: 1.182202935218811, accuracy: 80.51\n",
      "iteration: 2500, loss: 1.090092420578003, accuracy: 81.69\n",
      "iteration: 3000, loss: 1.018148422241211, accuracy: 82.63\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: loading dataset\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2 : making dataset iterable\n",
    "'''\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: create model class\n",
    "'''\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "'''\n",
    "STEP 4: instantiate model class\n",
    "'''\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: instantiate loss class\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: instantiate optimizer class\n",
    "'''\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 7: train the model\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # load images\n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to get output\n",
    "        # 100 x 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # calculate loss: softmax -> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # getting gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28)\n",
    "                # forward\n",
    "                outputs = model(images)\n",
    "                # get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # total number of labels\n",
    "                total += labels.size(0)\n",
    "                # total correct predictions\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100*correct.item()/total\n",
    "            \n",
    "            # print loss\n",
    "            print('iteration: {}, loss: {}, accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU Version**\n",
    "\n",
    "2 things must be on GPU \n",
    "- `model`\n",
    "- `tensors`\n",
    "\n",
    "Remember step 4 and 7 will be affected and this will be the same for all model building moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 500, loss: 1.871890902519226, accuracy: 67.89\n",
      "iteration: 1000, loss: 1.5203700065612793, accuracy: 76.31\n",
      "iteration: 1500, loss: 1.3976773023605347, accuracy: 79.15\n",
      "iteration: 2000, loss: 1.1869350671768188, accuracy: 81.0\n",
      "iteration: 2500, loss: 1.069792628288269, accuracy: 81.94\n",
      "iteration: 3000, loss: 1.1011682748794556, accuracy: 82.86\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: loading dataset\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2 : making dataset iterable\n",
    "'''\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: create model class\n",
    "'''\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "'''\n",
    "STEP 4: instantiate model class\n",
    "'''\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "'''\n",
    "STEP 5: instantiate loss class\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: instantiate optimizer class\n",
    "'''\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 7: train the model\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # load images\n",
    "        \n",
    "        ########################\n",
    "        #  USE GPU FOR TENSOR  #\n",
    "        ########################\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to get output\n",
    "        # 100 x 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # calculate loss: softmax -> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # getting gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                ########################\n",
    "                #  USE GPU FOR TENSOR  #\n",
    "                ########################\n",
    "                images = images.view(-1, 28*28).to(device)\n",
    "                \n",
    "                # forward\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # total correct predictions\n",
    "                ########################\n",
    "                #  USE GPU FOR TENSOR  #\n",
    "                ########################\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu()==labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted==labels).sum()\n",
    "            accuracy = 100*correct.item()/total\n",
    "            \n",
    "            # print loss\n",
    "            print('iteration: {}, loss: {}, accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Logistic regression** basics\n",
    "- **Problems** of **linear regression**\n",
    "- **In-depth** Logistic Regression\n",
    "    1. Get logits\n",
    "    2. Get softmax\n",
    "    3. Get cross-entropy loss\n",
    "- **Aim**: reduce cross-entropy loss\n",
    "- Built a **logistic regression model** in **CPU and GPU**\n",
    "    - Step 1: Load Dataset\n",
    "    - Step 2: Make Dataset Iterable\n",
    "    - Step 3: Create Model Class\n",
    "    - Step 4: Instantiate Model Class\n",
    "    - Step 5: Instantiate Loss Class\n",
    "    - Step 6: Instantiate Optimizer Class\n",
    "    - Step 7: Train Model\n",
    "- Important things to be on **GPU**\n",
    "    - `model`\n",
    "    - `tensors with gradients`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network with PyTorch\n",
    "## 1. About Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Logistic Regression Transition to Neural Networks\n",
    "\n",
    "#### Logistic Regression Review\n",
    "\n",
    "<img src=\"./images/lab2/cross_entropy_final_4.png\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define logistic regression model**\n",
    "\n",
    "Import our relevant torch modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we inspect the model, we would have an input size of 784 (derived from 28 x 28) and output size of 10 (which is the number of classes we are classifying from 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Problems\n",
    "- Can represent **linear** functions well\n",
    "    - $ y = 2x + 3$\n",
    "    - $ y = x_1 + x_2 $\n",
    "    - $ y = x_1 + 3x_2 + 4x_3 $\n",
    "- Cannot represent **non-linear** functions\n",
    "    - $ y = 4x_1 + 2x_2^2 +3x_3^3 $\n",
    "    - $ y = x_1x_2$\n",
    "       \n",
    "### 1.2 Introducing a Non-linear Function\n",
    "\n",
    "<img src=\"./images/lab2/logistic_regression_comparison_nn5.png\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "\n",
    "### Non-linear Function In-Depth\n",
    "- Function: takes a number & perform mathematical operation\n",
    "- Common Types of Non-linearity\n",
    "    - ReLUs (Rectified Linear Units)      \n",
    "    - Sigmoid     \n",
    "    - Tanh\n",
    "\n",
    "#### Sigmoid (Logistic)\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- Input number $\\rightarrow$ [0, 1]\n",
    "    - Large negative number $\\rightarrow$ 0\n",
    "    - Large positive number $\\rightarrow$ 1\n",
    "- Cons: \n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        - No signal to update weights $\\rightarrow$ **cannot learn**\n",
    "        - **Solution**: Have to carefully initialize weights to prevent this\n",
    "    2. Outputs not centered around 0 \n",
    "        - If output always positive $\\rightarrow$ gradients always positive or negative $\\rightarrow$ **bad for gradient updates** \n",
    "\n",
    "#### Tanh\n",
    "- $\\tanh(x) = 2 \\sigma(2x) -1$\n",
    "    - A scaled sigmoid function\n",
    "- Input number $\\rightarrow$ [-1, 1]\n",
    "- Cons: \n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        - No signal to update weights $\\rightarrow$ **cannot learn**\n",
    "        - **Solution**: Have to carefully initialize weights to prevent this\n",
    "\n",
    " \n",
    "#### ReLUs\n",
    "- $f(x) = \\max(0, x)$\n",
    "- Pros:\n",
    "    1. Accelerates convergence $\\rightarrow$ **train faster**\n",
    "    2. **Less computationally expensive operation** compared to Sigmoid/Tanh exponentials\n",
    "- Cons:\n",
    "    1. Many ReLU units \"die\" $\\rightarrow$ **gradients = 0** forever\n",
    "        - **Solution**: careful learning rate choice\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Feedforward Neural Network with PyTorch\n",
    "\n",
    "### Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)\n",
    "<img src=\"./images/lab2/nn1.png\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading MNIST Train Dataset\n",
    "**Images from 1 to 9**\n",
    "\n",
    "Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MINST/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:03, 3243695.80it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MINST/MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/MINST/MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MINST/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 64672.01it/s]                                                                                          \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MINST/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/MINST/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MINST/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 1111327.35it/s]                                                                                      \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MINST/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/MINST/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MINST/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 19416.33it/s]                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MINST/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/MINST/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='./data/MINST/',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data/MINST/',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch sizes and iterations**\n",
    "\n",
    "Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently.\n",
    "\n",
    "There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would:\n",
    "\n",
    "(1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors.\n",
    "\n",
    "(2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests!\n",
    "\n",
    "If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 100 images to the model and getting their respective predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epochs**\n",
    "\n",
    "An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations.\n",
    "\n",
    "If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "600 * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bringing batch size, iterations and epochs together**\n",
    "\n",
    "As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100.\n",
    "\n",
    "Because we want 5 epochs, we need a total of 3000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000 # total data / batch_size * epoch\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating our feedforward neural network**\n",
    "\n",
    "Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer.\n",
    "\n",
    "This translates to just 4 more lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # non-linearity\n",
    "        self.sigmoid = nn.Sigmoid\n",
    "        \n",
    "        # linear function\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate Model Class\n",
    "\n",
    "- **Input** dimension: **784** \n",
    "    - Size of image\n",
    "    - $28 \\times 28 = 784$\n",
    "- **Output** dimension: **10**\n",
    "    - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "- **Hidden** dimension: **100**\n",
    "    - Can be any number\n",
    "    - Similar term\n",
    "        - Number of neurons\n",
    "        - Number of non-linear activation functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating our model class**\n",
    "\n",
    "Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28).\n",
    "\n",
    "Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10.\n",
    "\n",
    "Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results.\n",
    "\n",
    "On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate Loss Class\n",
    "\n",
    "- Feedforward Neural Network: **Cross Entropy Loss**\n",
    "    - _Logistic Regression_: **Cross Entropy Loss**\n",
    "    - _Linear Regression_: **MSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss class**\n",
    "\n",
    "This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Instantiate Optimizer Class\n",
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$\n",
    "        - $\\theta$: parameters (our tensors with gradient accumulation capabilities)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    - **At every iteration, we update our model's parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer class**\n",
    "\n",
    "Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities.\n",
    "\n",
    "Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong.\n",
    "\n",
    "A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm.\n",
    "\n",
    "A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently.\n",
    "\n",
    "We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear layers' parameters**\n",
    "\n",
    "In a simple linear layer it's $Y = AX + B$, and our parameters are $A$ and bias $B$. \n",
    "    \n",
    "Hence, each linear layer would have 2 groups of parameters  $A$ and $B$. It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on $Y$, the output of our linear layers.\n",
    "    \n",
    "This would return a Python generator object, so you need to call list on the generator object to access anything meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001DD9AFB8A98>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters $A$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first linear layer parameters, $A_1$, would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n"
     ]
    }
   ],
   "source": [
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first linear layer bias parameters, $B_1$, would be of size 100 which is our hidden size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second linear layer is our readout layer, where the parameters $A_2$ would be of size 10 x 100. This is because our output size is 10 and hidden size is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise our readout layer's bias $B_1$ would just be 10, the size of our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below shows the interaction amongst our input $X$ and our linear layers' parameters $A_1$, $B_1$, $A_2$, and $B_2$ to reach to the final size of 10 x 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/lab2/nn1_params3.png\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Model\n",
    "- Process \n",
    "    1. Convert inputs to tensors with gradient accumulation capabilities\n",
    "    2. Clear gradient buffers\n",
    "    3. Get output given inputs \n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. parameters\n",
    "    6. Update parameters using gradients\n",
    "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7-step training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.6416046619415283. Accuracy: 85.76\n",
      "Iteration: 1000. Loss: 0.2638024687767029. Accuracy: 89.58\n",
      "Iteration: 1500. Loss: 0.2913622558116913. Accuracy: 90.54\n",
      "Iteration: 2000. Loss: 0.36239469051361084. Accuracy: 91.21\n",
      "Iteration: 2500. Loss: 0.2494616061449051. Accuracy: 91.62\n",
      "Iteration: 3000. Loss: 0.36123189330101013. Accuracy: 92.05\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)\n",
    "<img src=\"./images/lab2/nn1.png\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- **Step 3: Create Model Class**\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-layer FNN with Tanh Activation**\n",
    "\n",
    "The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 500, loss: 0.2872872054576874, accuracy: 91.01\n",
      "iter: 1000, loss: 0.2652204930782318, accuracy: 92.55\n",
      "iter: 1500, loss: 0.13556793332099915, accuracy: 93.41\n",
      "iter: 2000, loss: 0.2442370355129242, accuracy: 94.2\n",
      "iter: 2500, loss: 0.146214097738266, accuracy: 94.71\n",
      "iter: 3000, loss: 0.15331757068634033, accuracy: 95.08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# 1. load dataset\n",
    "train_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "# 2. make dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# 3. create model class\n",
    "class FFNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FFNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "# 4. instantiate model class\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100\n",
    "\n",
    "model = FFNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 5. instantiate loss class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6. instantiate optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 7. train the model\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # convert inputs to tensors with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # get gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # calculate acuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct.item() / total\n",
    "            \n",
    "            print('iter: {}, loss: {}, accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
    "<img src=\"./images/lab2/nn1.png\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- **Step 3: Create Model Class**\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-layer FNN with ReLU Activation**\n",
    "\n",
    "The only difference again is in using ReLU activation and it affects step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 500, loss: 0.22274595499038696, accuracy: 91.17\n",
      "iter: 1000, loss: 0.3316263258457184, accuracy: 92.85\n",
      "iter: 1500, loss: 0.22306746244430542, accuracy: 93.85\n",
      "iter: 2000, loss: 0.31199389696121216, accuracy: 94.53\n",
      "iter: 2500, loss: 0.19311437010765076, accuracy: 95.31\n",
      "iter: 3000, loss: 0.2146567404270172, accuracy: 95.87\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# 1. load dataset\n",
    "train_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "# 2. make dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# 3. create model class\n",
    "class FFNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FFNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "# 4. instantiate model class\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100\n",
    "\n",
    "model = FFNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 5. instantiate loss class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6. instantiate optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 7. train the model\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # convert inputs to tensors with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # get gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # calculate acuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct.item() / total\n",
    "            \n",
    "            print('iter: {}, loss: {}, accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
    "<img src=\"./images/lab2/nn2.png\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- **Step 3: Create Model Class**\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-layer FNN with ReLU Activation**\n",
    "\n",
    "This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 500, loss: 0.18185226619243622, accuracy: 91.13\n",
      "iter: 1000, loss: 0.1496511697769165, accuracy: 93.63\n",
      "iter: 1500, loss: 0.17544814944267273, accuracy: 94.78\n",
      "iter: 2000, loss: 0.1624055802822113, accuracy: 95.99\n",
      "iter: 2500, loss: 0.06035758927464485, accuracy: 96.73\n",
      "iter: 3000, loss: 0.07528278231620789, accuracy: 96.67\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# 1. load dataset\n",
    "train_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/MNIST/',\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "# 2. make dataset iterable\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# 3. create model class\n",
    "class FFNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FFNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# 4. instantiate model class\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100\n",
    "\n",
    "model = FFNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 5. instantiate loss class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6. instantiate optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 7. train the model\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # convert inputs to tensors with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # get gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # calculate acuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct.item() / total\n",
    "            \n",
    "            print('iter: {}, loss: {}, accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
    "<img src=\"./images/lab2/nn3.png\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- **Step 3: Create Model Class**\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-layer FNN with ReLU Activation**\n",
    "\n",
    "Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.23731116950511932. Accuracy: 91.3\n",
      "Iteration: 1000. Loss: 0.22314941883087158. Accuracy: 93.95\n",
      "Iteration: 1500. Loss: 0.10083617269992828. Accuracy: 95.32\n",
      "Iteration: 2000. Loss: 0.09337284415960312. Accuracy: 95.55\n",
      "Iteration: 2500. Loss: 0.05002380535006523. Accuracy: 96.6\n",
      "Iteration: 3000. Loss: 0.19129978120326996. Accuracy: 97.06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data/MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/MNIST/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Comments on FNNs\n",
    "- 2 ways to expand a neural network\n",
    "    - More non-linear activation units (neurons)\n",
    "    - More hidden layers \n",
    "- Cons\n",
    "    - Need a larger dataset\n",
    "        - Curse of dimensionality\n",
    "    - Does not necessarily mean higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Feedforward Neural Network with PyTorch (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/lab2/nn3.png\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "GPU: 2 things must be on GPU\n",
    "- `model`\n",
    "- `tensors with gradient accumulation capabilities`\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- **Step 4: Instantiate Model Class**\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- **Step 7: Train Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-layer FNN with ReLU Activation on GPU**\n",
    "\n",
    "Only step 4 and 7 of the CPU code will be affected and it's a simple change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.26560378074645996. Accuracy: 90.62\n",
      "Iteration: 1000. Loss: 0.20822454988956451. Accuracy: 94.25\n",
      "Iteration: 1500. Loss: 0.12326369434595108. Accuracy: 94.56\n",
      "Iteration: 2000. Loss: 0.10073315352201462. Accuracy: 96.04\n",
      "Iteration: 2500. Loss: 0.04586681351065636. Accuracy: 96.47\n",
      "Iteration: 3000. Loss: 0.07458092272281647. Accuracy: 97.14\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data/MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/MNIST/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Alternative Term of Neural Network**\n",
    ">\n",
    "> The alternative term is **Universal Function Approximator**. This is because ultimately we are trying to find a function that maps our input, $X$, to our output, $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Logistic Regression Problems**\n",
    "    - Cannot represent **non-linear** functions\n",
    "        - $ y = 4x_1 + 2x_2^2 +3x_3^3 $\n",
    "        - $ y = x_1x_2$\n",
    "- Introduced **Non-Linearity** to Logistic Regression to form a Neural Network\n",
    "- **Types** of Non-Linearity\n",
    "    - Sigmoid\n",
    "    - Tanh\n",
    "    - ReLU\n",
    "- Feedforward Neural Network **Models**\n",
    "    - Model A: 1 hidden layer (**sigmoid** activation)\n",
    "    - Model B: 1 hidden layer (**tanh** activation)\n",
    "    - Model C: 1 hidden layer (**ReLU** activation)\n",
    "    - Model D: **2 hidden** layers (ReLU activation)\n",
    "    - Model E: **3 hidden** layers (ReLU activation)\n",
    "- Models Variation in **Code**\n",
    "    - Modifying only step 3\n",
    "- Ways to Expand Models **Capacity**\n",
    "    - More non-linear activation units (**neurons**)\n",
    "    - More hidden **layers**\n",
    "- **Cons** of Expanding Capacity\n",
    "    - Need more **data**\n",
    "    - Does not necessarily mean higher **accuracy**\n",
    "- **GPU** Code\n",
    "    - 2 things on GPU\n",
    "        - **model**\n",
    "        - **tensors with gradient accumulation capabilities**\n",
    "    - Modifying only **Step 4 & Step 7**\n",
    "- **7 Step** Model Building Recap\n",
    "    - Step 1: Load Dataset\n",
    "    - Step 2: Make Dataset Iterable\n",
    "    - Step 3: Create Model Class\n",
    "    - Step 4: Instantiate Model Class\n",
    "    - Step 5: Instantiate Loss Class\n",
    "    - Step 6: Instantiate Optimizer Class\n",
    "    - Step 7: Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *References*\n",
    "[1] [DOI](https://zenodo.org/badge/139945544.svg)(https://zenodo.org/badge/latestdoi/139945544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
